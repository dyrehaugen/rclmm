[["index.html", "Climate Models 1 Climate Change", " Climate Models Dyrehaugen Web Notebook 2023-12-26 1 Climate Change The issue of Climate Change is treated across several web notebooks: Climate System (github) (geek) (loc) Climate Models (github) (geek) (loc) Climate Impacts (github) (geek) (loc) Climate Actions (github) (geek) (loc) You are now visiting the Climate Models Web Notebook. The theme of Climate Change is large and split into multiple Dyrehaugen Workbooks. Here we treat Climate Models. Climate is someone else’s Weather Climate Crisis and Economic Development are one and the same problem The neoliberal solution to climate change is to hope that somehow it will become profitable to save the planet. This will not work. All models are wrong, but some are useful. "],["climate-models.html", "2 Climate Models 2.1 University of Chicago 2.2 Mental Picture of Greenhouse Effect 2.3 Political IAMs 2.4 CAP6 2.5 Coupled Climate-Social Models 2.6 DICE Climate Model 2.7 E3ME-FTT-GENIE 2.8 EZ-Climate Model 2.9 FAIR Climate Model 2.10 GCAM 2.11 Global Calculator", " 2 Climate Models Overview text on selected Climate Models 2.1 University of Chicago University of Chicago Climate Models 2.2 Mental Picture of Greenhouse Effect Benestad The popular picture of the greenhouse effect emphasises the radiation transfer but fails to explain the observed climate change. The earth’s climate is constrained by well-known and elementary physical princi- ples, such as energy balance, flow, and conservation. Green- house gases affect the atmospheric optical depth for infrared radiation, and increased opacity implies higher altitude from which earth’s equivalent bulk heat loss takes place. Such an increase is seen in the reanalyses, and the outgoing long- wave radiation has become more diffuse over time, consistent with an increased influence of greenhouse gases on the vertical energy flow from the surface to the top of the atmosphere. The reanalyses further imply increases in the overturning in the troposphere, consistent with a constant and continuous vertical energy flow. The increased overturning can explain a slowdown in the global warming, and the association between these aspects can be interpreted as an entanglement between the greenhouse effect and the hydrological cycle, where reduced energy transfer associated with increased opacity is compensated by tropospheric overturning activity. Benestad (2017) (pdf) William Kininmonth’s ‘rethink’ on the greenhouse effect for The Global Warming Policy Foundation. He made some rather strange claims, such as that the Intergovernmental Panel on Climate Change (IPCC) allegedly should have forgotten that the earth is a sphere because “most absorption of solar radiation takes place over the tropics, while there is excess emission of longwave radiation to space over higher latitudes”. Kininmonth’s calculations are based on wrong assumptions. When looking at the effect of changes in greenhouse gases, one must look at how their forcing corresponds to the energy balance at the top of the atmosphere. But Kininmonth instead looks at the energy balance at the surface where a lot of other things also happen, and where both tangible and latent energy flows are present and make everything more complicated. It is easier to deal with the balance at the top of the atmosphere or use a simplified description that includes convection and radiation. Another weak point is Kininmonth’s assumption of the water vapour being constant and at same concentrations as in the tropics over the whole globe. Focusing on the tropics easily gives too high values ​​for water vapour if applied to the whole planet. Another surprising claim that Kininmonth made was that ocean currents are the only plausible explanation for the warming of the tropical reservoir, because he somehow thinks that there has been a reduction in the transport of heat to higher latitudes due to a mysterious slow down of ocean currents. It is easy to check trends in sea surface temperatures and look for signs that heat transport towards higher latitudes has weakened. Such a hypothetical slowdown would suggest weaker ocean surface warming in the high latitudes, which is not supported by data. Benestad (2022) New misguided interpretations of the greenhouse effect from William Kininmonth Is it possible to provide a simple description that is physically meaningful and more sophisticated than the ‘blanket around earth’ concept? The starting point was to look at the bulk – the average – heat radiation and the total energy flow. I searched the publications back in time, and found a paper on the greenhouse effect from 1931 by the American physicist Edward Olson Hulburt (1890-1982) that provided a nice description. The greenhouse effect involves more than just radiation. Convection also plays a crucial role. How does the understanding from 1931 stand up in the modern times? I evaluated the old model with modern state-of-the-art data: reanalyses and satellite observations. With an increased greenhouse effect, the optical depth increases. Hence, one would expect that earth’s heat loss (also known as the outgoing longwave radiation, OLR) becomes more diffuse and less similar to the temperature pattern at the surface. An analysis of spatial correlation between heat radiation estimated for the surface temperatures and that at the top of the atmosphere suggests that the OLR has become more diffuse over time. The depth in the atmosphere from which the earth’s heat loss to space takes place is often referred to as the emission height. For simplicity, we can assume that the emission height is where the temperature is 254K in order for the associated black body radiation to match the incoming flow of energy from the sun. Additionally, as the infrared light which makes up the OLR is subject to more absorption with higher concentrations of greenhouse gases (Beer-Lambert’s law), the mean emission height for the OLR escaping out to space must increase as the atmosphere gets more opaque. There has been an upward trend in the simple proxy for the emission height in the reanalyses. This trend seems to be consistent with the surface warming with the observed lapse rate (approximately -5K/km on a global scale). One caveat is, however, that trends in reanalyses may be misleading due to introduction of new observational instruments over time (Thorne &amp; Vose, 2010). Finally, the energy flow from the surface to the emission height must be the same as the total OLR emitted back to space, and if increased absorption inhibits the radiative flow between earth’s surface and the emission height, then it must be compensated by other means. The energy flow is like the water in a river: it cannot just appear or disappear; it flows from place to place. In this case, the vertical energy flow is influenced by deep convection, which also plays a role in maintaining the lapse rate. Benestad (2016) What is the best description of the greenhouse effect? 2.3 Political IAMs Peng Memo Similar to many economic tools developed decades ago, IAMs are built on an oversimplified logic: that people are rational optimizers of scarce resources. ‘Agents’ make decisions that maximize the benefits to a country or society2. Price adjustments — for example, a carbon tax — or constraints on polluting technologies alter the agents’ incentives, yielding changes in behaviour that alter economies and emissions4. In reality, human choice is a darker brew of misperception and missed opportunity, constrained by others’ decisions. Researchers in sociology, psychology and organizational behaviour have long studied human behaviours. They explore why people stick with old, familiar technologies even when new ones are much superior, for example. This kind of research can also explain why the passion of mass movements, such as the global climate-strike movement, Fridays for Future, is hard to understand based on just individual costs and benefits, yet it can have powerful effects on policy. To get IAMs to reflect social realities and possibilities, one should look to the field of political economy. Eight political economy insights Data improve models’ relevance to policy and investment choices. • Access to capital can be constrained by risk-averse investors who fear unpredictable changes in policy, hampering low-carbon energy transitions. • The design and type of a policy instrument, such as whether to subsidize green technologies or tax polluting industries, can be influenced by which interest groups are mobilized. • Carbon lock-in and stranding of fossil-based energy assets might limit the degree to which emissions can deviate from their previous trajectory, without interventions that can weaken the power of incumbent polluters. • Unequal costs and benefits of climate policies accrue to different economic, racial and religious groups, which can affect policies’ moral and political acceptability. • Public opinion might facilitate stronger action to tackle climate change. • Confidence in political institutions or lack of it can influence the public’s willingness to support actions that reduce emissions. • Trade and investment policies can expand the markets for new green technology, leading to lower costs and more political support. • Competence of government influences a state’s ability to intervene in markets, make choices and alter the cost of deploying capital. Peng (2021) Climate policy models need to get real about people — here’s how 2.4 CAP6 Carbon Asset Procing Model -AR6 Bauer/Proistosecu/Wagner Abstract Valuing the cost of carbon dioxide (CO 2 ) emissions is vital in weighing viable approaches to climate policy. Conventional approaches to pricing CO 2 evaluate trade-offs between sacrificing consumption now to abate CO 2 emissions versus growing the economy, therefore enlarging one’s financial resources to pay for climate damages later, all within a standard Ramsey growth frame- work. However, these approaches fail to comprehensively incorporate decision making under risk and uncertainty in their analysis, a limitation especially relevant considering the tail risks of climate impacts. Here, we take a financial-economic approach to carbon pricing by developing the Carbon Asset Pricing model – AR6 (CAP6), which prices CO 2 emissions as an asset with negative returns and allows for an explicit representation of risk in its model structure. CAP6’s emissions projec- tions, climate damage functions, climate emulator, and mitigation cost function are in line with the sixth assessment report (AR6) from the Intergovernmental Panel on Climate Change (IPCC). The economic parameters (such as the discount rate) are calibrated to reflect recent empirical work. We find that in our main specification, CAP6 provides support for a high carbon price in the near-term that declines over time, with a resulting ‘optimal’ expected warming below the 1.5 ◦ C target set forth in the Paris agreement. We find that, even if the cost of mitigating CO 2 emissions is much higher than what is estimated by the IPCC, CAP6 still provides support for a high carbon price, and the resulting ‘optimal’ warming stays below 2 ◦ C (albeit warming does exceed 1.5 ◦ C by 2100). Incorporating learning by doing further lowers expected warming while making ‘optimal’ policy more cost effective. By decomposing sources of climate damages, we find that risk associated with slowed economic growth has an outsized influence on climate risk assessment in comparison to static-in-time estimates of climate damages. In a sensitivity analysis, we sample a range of discount rates and technological growth rates, and disentangle the role of each of these assumptions in determining central estimates of CAP6 output as well as its uncertainty over time. We find that central estimates of carbon prices are sensitive to assumptions around emissions projections, whereas estimates of warming, CO 2 concentrations, and economic damages are largely robust to such assumptions. We decompose the uncertainty in CO 2 prices, temperature rise, atmospheric CO 2 levels, and economic damages over time, finding that individual preferences control price uncertainty in the near term, while rates of technological change drive price uncertainty in the distant future. The influence of individual preferences on temperature rise, CO 2 concentrations, and economic damages is realized for longer than in the case of CO 2 prices owing to the consequences of early inaction. Taken in totality, our work highlights the necessity of early and stringent action to mitigate CO 2 emissions in addressing the dangers posed by climate change. Bauer/Proistosecu/Wagner Memo Conventional IAMs (such as the dynamic integrated climate-economy, or DICE evaluate climate change impacts within the context of a standard Ramsey growth economy. In this approach, one considers tradeoffs between emitting CO 2 and incurring damages both now and, largely, in the future, versus abating CO 2 emissions now for some cost. The resulting benefit-cost analysis results in a presently-low (∼ $40 in the case of DICE– 2016R) and rising ‘optimal’ price over time, with significant warming (∼ 4 ◦ C) by 2100. It is notable that DICE’s suggested ‘optimal’ warming projections are larger than the warming target of 1.5 ◦ C established in the Paris Agreement. A limitation of Ramsey growth IAMs is that they lack a comprehensive description of decision-making under uncertainty, a feature of many financial economics models. This is important, as climate change projections are inherently probabilistic, with low probability, extreme impact outcomes presenting the most significant risk to the climate-economic system (i.e., a potentially high climate response to emissions leading to rapid warming). Additionally, many complex risks associated with climate change cannot currently be fully quantified and are therefore excluded from economic analyses, despite impacting the overall risk landscape of climate impacts. An example of this are climate tipping points, which have been argued to lead to rapid environmental degradation [61, 5] and increase the SCC [20]. These “deep” uncertainties in the impacts of climate change have led some to advocate for an “insurance” to be taken out against high climate damages. Ramsey growth models do not allow for such considerations in determining their policy projections. Put differently: Ramsey growth IAMs do not allow individuals to ‘hedge’ against climate catastrophe. Recently, financial asset pricing models have been introduced in an effort to understand how risk im- pacts climate policy decision making [18, 19, 10]. Such models take a fundamentally different approach than conventional, Ramsey growth IAMs. Instead of computing the “shadow price” of CO 2 (which is to say, the price of CO 2 implied from distortions in consumption and economic utility owing to climate damages), financial asset pricing models compute the price of CO 2 directly, treating CO 2 as an asset with negative returns. The result is not the SCC of yore, but rather, a direct ‘optimal’ price for each ton of CO 2 emitted. Here, we introduce the Carbon Asset Pricing model – AR6 (abbreviated to CAP6 herein), a climate- economy IAM that builds on previous financial asset pricing climate-economy models. CAP6 embeds a representative agent in a binomial, path dependent tree that models decision making under uncertainty. Prior to optimization, a number of potential trees are generated via sampling climate and climate impacts uncertainty that the agent traverses depending on emissions abatement choices. The present-day Epstein-Zin (EZ) utility of consumption is optimized to determine the ‘optimal’ emissions abatement policy. EZ utility allows for the separation of risk aversion across states of time and states of nature, a distinction theorized to play a significant role in climate policy. The climate component of the model utilizes an effective transient climate response to emis- sions (TCRE) to map cumulative emissions to global mean surface temperature (GMST) anomaly, and a simple carbon cycle model to map CO 2 emissions to concentrations. We sample with equal probability three damage functions of different shape and scale, thus capturing both parametric and epistemic uncertainty in the damage function in our risk assessment. Finally, we formulate a new marginal abatement cost curve (MACC), providing a much-needed update to the McKinsey MACC. We find that the ‘optimal’ expected warming in the preferred calibration is in line with the 1.5 ◦ C of warming by 2100 target set forth in the Paris agreement. Furthermore, we find that even if we are pessimistic about the cost estimates provided by the IPCC, the preferred calibration of CAP6 still supports limiting warming to less than 2 ◦ C warming by 2100. We demonstrate the role of learning by doing by allowing for endogenous technological growth, and find that this decreases the overall costs of ‘optimal’ policy and lowers expected warming. We show that risk associated with slowing economic growth has an outsized influence on price path dynamics in comparison to static-in-time estimates of climate damages, and argue this is a general feature of CAP6 ‘optimal’ price paths. Bauer (2023) Carbon Dioxide as a Risky Asset (SSRN) Bauer (2023) Financial modeling of climate risk supports stringent mitigation action. Working Paper (pdf) Review of Bauer Nuccitelli For decades, economists believed immediate action to fight climate change would decimate the economy, but a new study adds to a growing body of research showing that the economic benefits of climate action outweigh the costs. The paper’s climate-economics model incorporates up-to-date estimates from the 2022 Sixth Intergovernmental Panel on Climate Change, or IPCC, report on climate-warming pollution, climate responses, resulting damages, and the costs of reducing those emissions. Its core conclusions are largely determined by three factors: the benefits of “learning by doing,” the steep economic costs of catastrophic climate change, and a more realistic “discount rate.” Accounting for these factors reveals that any possible savings from current inaction would not generate enough funds over time to fix potential damage from climate catastrophe. Nuccitelli (2023) Drastic climate action is the best 2.5 Coupled Climate-Social Models Moore Abstract The ambition and effectiveness of climate policies will be essential in determining greenhouse gas emissions and, as a consequence, the scale of climate change impacts 1,2 . However, the socio-politico-technical processes that will determine climate policy and emissions trajectories are treated as exogenous in almost all climate change modelling 3,4 . Here we identify relevant feedback processes documented across a range of disciplines and connect them in a stylized model of the climate–social system. An analysis of model behaviour reveals the potential for nonlinearities and tipping points that are particularly associated with connections across the individual, community, national and global scales represented. These connections can be decisive for determining policy and emissions outcomes. After partly constraining the model parameter space using observations, we simulate 100,000 possible future policy and emissions trajectories. These fall into 5 clusters with warming in 2100 ranging between 1.8 °C and 3.6 °C above the 1880–1910 average. Public perceptions of climate change, the future cost and effectiveness of mitigation technologies, and the responsiveness of political institutions emerge as important in explaining variation in emissions pathways and therefore the constraints on warming over the twenty-first century. Moore Memo Moore (2022) Determinants of emissions pathways in the coupled climate–social system (pdf) Volts Podcast: Fran Moore 2.6 DICE Climate Model Like some early IAMs, such as William Nordhaus’ Nobel Prize-winning DICE model, the program we built is basic enough to run on an ordinary laptop in less than a second. However, like Nordhaus’ model, ours is far too simple to be used in real life. (Such limitations never stop economists.) Pendergrass If you enter the climatic conditions of Venus into the DICE integrated assessment model, the economy stills grows nicely and everyone lives happily ever after. Independent of the normative assumptions of inequality aversion and time preferences, the Paris agreement constitutes the economically optimal policy pathway for the century. Authors claim they show this by incorporating a damage-cost curve reproducing the observed relation between temperature and economic growth into the integrated assessment model DICE. Glanemann (2021) DICE Paris CBA (pdf) 2.6.1 The failure of Dice Economics If there is one climate economist who is respected above all others, it’s William Nordhaus of Yale, who won the Econ Nobel in 2018 “for integrating climate change into long-run macroeconomic analysis.” The prize specifically cited Nordhaus’ creation of an “integrated assessment model” for analyzing the costs of climate change. The most famous of these is the DICE Model, used by the Environmental Protection Agency. But the DICE Model, or at least the version we’ve been using for years, is obviously bananas. Noah Smith For other economists look here Catie Hausman Twitter Thread 2.6.2 Critique of the ‘Nordhaus School’ Ketcham An uncharitable view of the work of climate economists in the Nordhaus school is that they offer a kind of sociopathy as policy prescription. (Long article with many references to why DICE is nonsense). Ketcham (2023) When Idiot Savants Do Climate Economics - How an elite clique of math-addled economists hijacked climate policy. 2.6.3 Decarbonization in DICE Kopp DICE is a model of the macroeconomy, and the way it thinks of decarbonization investment is wonky. In DICE, the incremental cost of decarbonization is essentially a form of consumption; it is a zero-productivity investment that competes with productive capital investment and leads to less economic growth. This is ok if the reference state is a perfect market with no market failures aside from climate change, but it’s not. Investing in technology with fast learning curves (e.g., solar) is likely more productive than investing in more stagnant tech. If investment can be diverted from lower- or negative-productivity destinations (e.g., crypto or another bubble that does not leave substantial capital behind when it bursts), that’ll be positive for growth, too. And that’s aside from the fact that most of the time the economy isn’t at full employment, and so public investment in practically anything will increase output. That the bottom-up models miss this may be second order, because they aren’t trying to address the question of how much we should mitigate, just how different mitigation targets would play out on the composition of the energy and ag sectors. But it seems clearly first order if one is trying to optimize the macroeconomic tradeoffs between climate change and mitigation investment. I definitely wouldn’t consider a DICE-class top-down model with a polynomial abatement cost curve technical overkill. The problem I have with it is that decarbonization dollars cause no economic stimulus, unlike any other capital investment in the model. Kopp (2023) Decarbonization in DICE (X thread) Kotchen (2023) The costs of “costless” climate mitigation 2.7 E3ME-FTT-GENIE Mercure Abstract A high degree of consensus exists in the climate sciences over the role that human interference with the atmosphere is playing in changing the climate. Following the Paris Agreement, a similar consensus exists in the policy community over the urgency of policy solutions to the climate problem. The context for climate policy is thus moving from agenda setting, which has now been mostly established, to impact assessment, in which we identify policy pathways to implement the Paris Agreement. Most integrated assessment models currently used to address the economic and technical feasibility of avoiding climate change are based on engineering perspectives with a normative systems optimisation philosophy, suitable for agenda setting, but unsuitable to assess the socio-economic impacts of realistic baskets of climate policies. Here, we introduce a fully descriptive, simulation-based integrated assessment model designed speciﬁcally to assess policies, formed by the combination of (1) a highly disaggregated macro- econometric simulation of the global economy based on time series regressions (E3ME), (2) a family of bottom-up evolutionary simulations of technology diffusion based on cross-sectional discrete choice models (FTT), and (3) a carbon cycle and atmosphere circulation model of intermediate complexity (GENIE). We use this combined model to create a detailed global and sectoral policy map and scenario that sets the economy on a pathway that achieves the goals of the Paris Agreement with &gt;66% probability of not exceeding 2 C of global warming. We propose a blueprint for a new role for integrated assessment models in this upcoming policy assessment context. Mecure Memo The E3ME-FTT-GENIE 2 model is a simulation-based integrated assessment model that is fully descriptive, in which dynamical (time-dependent) human or natural behaviour is driven by empirically-determined dynamical relationships. At its core is the macroeconomic model E3ME, which represents aggregate human behaviour through a chosen set of econometric relationships that are regressed on the past 45 years of data and are projected 35 years into the future. The macroeconomics in the model determine total demand for manufactured products, services and energy car- riers. Meanwhile, technology diffusion in the FTT family of tech- nology modules determines changes in the environmental intensity of economic processes, including changes in amounts of energy required for transport, electricity generation and household heating. Since the development and diffusion of new technologies cannot be well modelled using time-series econometrics, cross- sectional datasets are used to parameterise choice models in FTT. Finally, greenhouse gas emissions are produced by the combustion of fuels and by other industrial processes, which interfere with the climate system. Natural non-renewable energy resources are modelled in detail with a dynamical depletion algorithm. And ﬁnally, to determine the climate impacts of chosen policies, E3ME- FTT global emissions are fed to the GENIE carbon cycle-climate system model of intermediate complexity. This enables, for instance, policy-makers to determine probabilistically whether or not climate targets are met. Mercure (2018) Environmental impact assessment for climate change policy with the simulation-based integrated assessment model E3ME-FTT-GENIE (pdf) Mercure Abstract A key aim of climate policy is to progressively substitute renewables and energy efficiency for fossil fuel use. The associated rapid depreciation and replacement of fossil-fuel-related physical and natural capital entail a profound reorganization of indus- try value chains, international trade and geopolitics. Here we present evidence confirming that the transformation of energy systems is well under way, and we explore the economic and strategic implications of the emerging energy geography. We show specifically that, given the economic implications of the ongoing energy transformation, the framing of climate policy as economically detrimental to those pursuing it is a poor description of strategic incentives. Instead, a new climate policy incen- tives configuration emerges in which fossil fuel importers are better off decarbonizing, competitive fossil fuel exporters are better off flooding markets and uncompetitive fossil fuel producers—rather than benefitting from ‘free-riding’—suffer from their exposure to stranded assets and lack of investment in decarbonization technologies. Mercure (2021) Reframing incentives for climate policy action (pdf) Espargne Annex I on E3ME-FTT-GENIE Integrated Assessment Model E3ME-FTT-GENIE is an integrated energy-technology-economy-climate simulation model used to assess the impacts of various types of policies, for various types of stakeholders including governments (EU Commission, national governments). The model specializes in, but is not exclusively used for, environmental, energy and climate policy, as well as labor markets. The model joins up an analysis of detailed technology diffusion dynamics for carbon-intensive sectors in FTT (Future Technology Transformations) with detailed and highly disaggregated macroeconomics in E3ME (Energy-Economy-Environment MacroEconometric model), and a fully-fledged climate and carbon cycle simulation of intermediate complexity in GENIE (Grid Enabled Integrated Earth system model). Of interest here are E3ME and FTT and the underlying detailed global energy system model. For a complete description of the model equations and dynamics, we refer the reader to Mercure et al.(2018a), including for a description of the climate simulation integration, which we omit here. Macroeconomic evolution in E3ME E3ME is a demand-driven macroeconometric model, based on a standard social accounting matrix with input- output relationships, bilateral trade relationships, and econometric equations describing the economic behavior of agents parameterized on time series from 1970 to the present. The model is disaggregated into 70 regions (including all G20 nations) and 43 (70) sectors of industry, for countries outside of the EU (inside of the EU). Econometric relationships are used to project the evolution of econometric variables up to 2070. The model manual (Cambridge Econometrics, 2022) is available online. A detailed list of all equations in E3ME is given in Mercure et al.(2018a). The model Is demand-driven, which means that it does not operate on the basis of production functions nor utility optimization. The model does not assume full employment of labor, physical and financial capital, but instead, assumes the existence of levels of resource use below full capacity (measured as unemployment and the output gap). In contrast to standard general equilibrium models, the consumption of agents by product type is first determined econometrically on the basis of prices, disposable income, population, and patterns of expenditure. The input-output relationships are then used to determine final and intermediate production as well as the demand for investment goods. Investment is determined econometrically on the basis of past economic activity, prices of capital assets and levels of capacity use. Employment and hours worked is determined on the basis of economic activity. Imports and exports are determined on the basis of price differentials between domestic and foreign goods by sector. Innovation is represented across the model through technology progress factors determined on the basis of cumulated past investment by sector. These indicators are integrated through various econometric equations, in particular domestic and export prices. The accumulation of capital in every sector is assumed to lead to production cost reductions, where the regression parameter is related to an effective sector-wide rate of learning-by-doing. Resulting price reductions determine the relative competitiveness of every sector-region. GDP is calculated on the basis of the sum of value added across the economy, where intermediate and final production in every sector is endogenously determined from levels of consumption. However, for consistency with other models, sectoral output is calibrated in the baseline scenario to match OECD and national economic projections. Energy sector module in E3ME Particular focus is adopted in E3ME towards estimating energy demand in physical units, by type of energy carrier, for all sectors and types of fuel users, on the basis of energy balance time series from the International Energy Agency. The final demand for energy carriers is determined for 22 types of final energy users (including industrial users, transport and non-energy types of use) for 12 types of fuels (incl. oil, coal, gas, electricity, biofuels). This allows to accurately estimate greenhouse gas emissions in all scenarios. Econometric estimations of energy use are made on the basis of sectoral economic activity and substitution between sectors. Technology diffusion in FTT While the above approach for modeling total energy demand by energy carrier is comprehensive, which ensures matching known greenhouse emission levels, the use of elasticities of substitution is less than accurate for fuel users in which technological change is the major driver of substitution. Instead, it is well known that an approach involving technological diffusion processes is much more satisfactory and allows to reproduce observed data. Furthermore, for technological changes, while price differentials incentivize substitutions of technologies across fuels, the use of fuel is not just simply related to price differentials but depends on a complex process of technology adoption by agents and the survival of technological stocks and fleets. The FTT model was created to represent the technological diffusion process in detail, on the basis of individual technologies currently available on the market, currently for power generation (Mercure et al., 2014), road transport (Lam and Mercure, 2021), heat (Knobloch et al., 2019) in buildings and steelmaking. This includes for example coal plants and solar panels for power generation, petrol and electric vehicles for road transport, gas boilers and heat pumps for household heating and so on. A current total of 88 technologies are represented (24 in power generation, 30 in road transport, 10 in household heating, 24 routes in steelmaking). The model is a vintage capital model that essentially represents fleets of technological items that agents purchase or invest in, each of which age and depreciate over time, with a turnover determined by technology- specific survival functions (or rates of life expectancy). For instance, cars survive on average for 11 years while coal plants survive for 40 years. This suggests that over 25 years, the vehicle fleet turns over entirely, while technological change is slower in power generation. Technological choice is represented on the basis of heterogeneous agents making comparisons between available technologies. The explicit assumption is made that the availability of technologies to agents is proportional to their prevalence in markets (the proportion of agents having access to technology A is proportional to the market share of that technology in markets). It is well known in sociology that agent investment or purchasing decisions are strongly determined by visual influence. This visual or peer influence effect is a way that agents have to reduce uncertainty when facing decisions to adopt new practices, and leads to the widely observed S-shaped profile of technological diffusion (Rogers, 2010). Rates of technological uptake in FTT are calibrated against historically observed diffusion rates, ensuring consistency between history and projections. The agent choice representation in FTT involves a comparison of a relevant levelized cost metric for each market (e.g., $/MWh in power generation, $ per km driven in road transport). Each technology is characterized by its particular learning-by-doing rate, which drives its cost down with cumulative investment. However, exogenous policies also influence rates of technological uptake, including technology-specific subsidies, the carbon price/tax, other taxes, bans and regulations as well as public/private procurement/investment. Fuel use is determined on the basis of technological compositions in each FTT sector. E3ME supplies FTT with total demand by FTT sector (power, transport, heat and steelmaking currently), and in return, FTT supplies E3ME with prices, investment, fuel use by fuel type and public income or expenditure through policy initiatives. The power model however does not model in detail the structure of electricity markets. The model has a representation of electricity storage, capacity factors, load bands and output allocation between different producing technologies according to auction by the network regulator. However, we have not carried out systematic studies of the different possible market clearing rules that could conceivably be adopted by regulators in different countries. We assume that electricity prices approximately reflect average costs of electricity production across the technology fleet in each country. Fossil fuel asset module Economic activity in fossil fuel production is strongly dependent on regional competitiveness in those sectors. This level of competitiveness is not straightforward to determine accurately from national accounts data. It is more effectively determined by using data on fossil fuel production by region. The model uses a detailed database of stocks of fossil fuels by region specified as distributed along a production cost variable. For oil and gas, this was determined using the Rystad database, which documents over 40,000 oil and gas assets worldwide. Coal reserves are determined similarly but given the ubiquity of coal resources worldwide at low extraction costs, the model uses less detailed data collected from various sources. Rystad provides 2P reserves and resources for each asset along with a breakeven cost value. The model assumes that each asset produces if and only if it is profitable at each time period (this may or may not always be accurate, as stopping production when it is unprofitable poses challenges in some contexts). The model uses the Rystad data to determine which asset produces and which asset is idle according to the price of oil and gas, and thus searches through the database to determine the prices of oil and gas that clear the demand each simulated year. This means that for instance, in scenarios of peaking and declining oil demand, some oil wells stop production and become stranded where the breakeven cost is high (e.g., tar sands in Canada), while others remain in production until they are depleted where the breakeven cost is low (e.g., conventional oil in OPEC countries). This calculation makes it possible to determine in detail production profiles for each E3ME country in each scenario, and these output profiles strongly affect economic activity for oil producers as it affects their exports and balance of trade. Conversely, this calculation indirectly influences oil importing countries as it redresses their trade balance through reduced imports. Thus, this model is a major source of structural change in the economy. Climate policies and scenarios In this paper, we make use of a number of policy instruments to simulate decarbonization to limit climate change to well below 2°C.The policies are exclusively instruments that are common and used by governments worldwide, including: carbon taxes, fuel taxes, technology subsidies, public investments, fuel blend mandates, vehicle mandates, scrappage schemes. In earlier work, we showed that in a model simulating non-linear technology diffusion processes such as in FTT, policies can produce complementarity effects where the overall outcome is more than the sum of the effects of the individual policies. Notably, carbon taxes and technology subsidies tend to work well with mandate policies (where manufacturers are required to market a proportion of low-carbon technologies), since mandates expand the choice options that consumers face, while the taxes or subsidies stabilize choices towards these new options. Taxes on their own work less well if choice is limited in which case consumers may be forced by circumstances to pay the taxes without changing their behavior. Mandates on their own work less well if the technologies pushed into the market fail to become cost competitive (Lam and Mercure, 2022; Mercure et al. 2014; Knobloch et al., 2019). It is also noteworthy that technology compositions in different countries are generally completely different, which often means that effective policy mixes tend to vary depending on circumstances. Some countries are endowed with largely low-carbon electricity sectors, while other countries find themselves well ahead of others in terms of low-carbon technology compositions as a result of past policies. Taking advantage of synergies explored in earlier work, and building on the policy mixes used in Mercure et al. (2021) and Nijsse et al. (2023), we constructed independent policy mixes in each of the 71 countries represented in the model. While they differ in each case, they build upon the following approach Cross-sectoral policies: • Carbon price that gradually increases over time to around $200/tCO2 in 2050 and covers the power sector and industrial activities, but not personal transport nor residential heat (as is currently the case in most countries). • Energy efficiency regulations for curbing energy use in sectors not modelled in FTT Power sector: • Feed-in tariffs (or contracts for difference) for wind power, but no policy usually needed for solar • Capital cost subsidies for technologies such as geothermal, hydro, carbon capture, nuclear and other low-carbon options • We assume implicitly that market regulations change to allow renewables to receive fair remuneration (e.g., reforming marginal cost pricing where it exists) • Ban on building new coal plants by 2030, and for gas plants by 2040. Road transport: • Ownership/purchase taxes for conventional vehicles • Subsidies on electric vehicles (we are not currently modelling hydrogen vehicles) • Electric vehicle mandates in the early years to increase numbers on roads • Biofuel blends • Bans on conventional vehicles in 2030 or on dates announced in various countries • We assume that charging infrastructure diffuses at the same pace as electric vehicles Household heating • Heating fuel taxes • Subsidies on heat pumps, solar heaters and other low-carbon options • Mandates on heat pumps and other low-carbon options Steelmaking • Public investment in hydrogen steel demonstration plants to attract private investment. This is modelled similarly to a mandate, in which industry is required to build capacity for low-carbon steel, part-funded by the public sector • Capital cost subsidies • We assume declining costs for hydrogen as inputs. To generate a scenario of global decarbonization, we searched policy space and adjusted the stringency of the policies to achieve net-zero in countries in which such a pledge has been made (2050 for the EU, Japan and Korea, 2060 for China, 2070 for India), and adjusted the policy stringency for the rest of the world to achieve net-zero by between 2050 and 2060. We note that there are very large numbers of equivalent policy mixes with which such emission reductions could be achieved in the model, but that carbon pricing on its own does not achieve those targets. Espargne (2023) Cross-Border Risks of a Global Economy in Mid-Transition 2.8 EZ-Climate Model Some text on EZClimate 2.9 FAIR Climate Model The FAIR model satisfies all criteria set by the NAS for use in an SCC calculation. 22 Importantly, this model generates projections of future warming that are consistent with comprehensive, state- of-the-art models and it can be used to accurately characterize current best understanding of the uncertainty regarding the impact that an additional ton of CO 2 has on global mean surface temperature (GMST). Finally, FAIR is easily implemented and transparently documented, 23 and is already being used in updates of the SCC. 24 A key limitation of FAIR and other simple climate models is that they do not represent the change in global mean sea level rise (GMSL) due to a marginal change in emissions. Carleton Greenstone (2021) Updating SCC (pdf) FAIR 2.10 GCAM Global Change Analysis Model JGCRI is the home and primary development institution for the Global Change Analysis Model (GCAM), an integrated tool for exploring the dynamics of the coupled human-Earth system and the response of this system to global changes. GCAM is a global model that represents the behavior of, and interactions between five systems: the energy system, water, agriculture and land use, the economy, and the climate. GCAM GCAM on GitHub GCAM Analysis of COP26 Pledges Over 100 nations have issued new commitments to reduce greenhouse gas emissions ahead of the United Nations Conference of the Parties, or COP26, currently underway in Glasgow. A new analysis published today in the journal Science assessed those new pledges, or nationally determined commitments (NDCs), and how they could shape Earth’s climate. The authors of the study, from institutions led by the Pacific Northwest National Laboratory and including Imperial College London, find the latest NDCs could chart a course where limiting global warming to 2°C and under within this century is now significantly more likely. Under pledges made at the 2015 Paris Agreement, the chances of limiting temperature change to below 2°C and 1.5°C above the average temperature before the industrial revolution by 2100 were 8 and 0 per cent, respectively. Under the new pledges – if they are successfully fulfilled and reinforced with policies and measures of equal or greater ambition – the study’s authors estimate those chances now rise to 34 and 1.5 percent, respectively. If countries strike a more ambitious path beyond 2030, those probabilities become even more likely, rising to 60 and 11 percent, respectively. Further, the chance of global temperatures rising above 4°C could be virtually eliminated. Under the 2015 pledges, the probability of such warming was more likely, at around 10 percent probability. The researchers used an open-source model called the Global Change Analysis Model (GCAM) to simulate a spectrum of emissions scenarios. Imperial College (2021) New climate pledges significantly more likely to prevent worst of global heating Dubash Abstract Discussions about climate mitigation tend to focus on the ambition of emission reduction targets or the prevalence, design, and stringency of climate policies. However, targets are more likely to translate to near-term action when backed by institutional machinery that guides policy development and implementation. Institutions also mediate the political interests that are often barriers to implementing targets and policies. Yet the study of domestic climate institutions is in its infancy, compared with the study of targets and policies. Existing governance literatures document the spread of climate laws (1, 2) and how climate policy-making depends on domestic political institutions (3–5). Yet these literatures shed less light on how states organize themselves internally to address climate change. To address this question, drawing on empirical case material summarized in table S1, we propose a systematic framework for the study of climate institutions. We lay out definitional categories for climate institutions, analyze how states address three core climate governance challenges—coordination, building consensus, and strategy development—and draw attention to how institutions and national political contexts influence and shape each other. Acontextual “best practice” notions of climate institutions are less useful than an understanding of how institutions evolve over time through interaction with national politics. Dubash (2021) National climate institutions complement targets and policies (Science) 2.11 Global Calculator Used by Kuhnhenn - STS - Heinrch Böll Stiftung GlobalCalculator Global Calculator Tool "],["hadcm3b.html", "3 HadCM3B 3.1 Greening of Sahara 3.2 Mimi Framework 3.3 MODTRAN 3.4 Monash Climate Model 3.5 PAGE 3.6 Bern Simple Climate Model (BernSCM) 3.7 NorESM", " 3 HadCM3B Armstrong The Hadley Centre Coupled Model 3 Bristol (HadCM3B) is a coupled climate model that consists of a 3d dynamical atmospheric component with a resolution of 2.5° × 3.75°, 19 vertical levels, and a 30 minute timestep89, and an ocean model with resolution of 1.25° × 1.25°, 20 vertical levels and a 1-hour timestep. Levels have a finer resolution towards the Earth surface. The model is a variant of HadCM3 that has been developed at the University of Bristol. Despite its relative old age, the model has been shown to accurately simulate the climate system and remains competitive with more modern climate models. A key advantage of the model is its computationally efficiency, which permits long simulations and large ensemble studies. We also utilise the atmosphere-only version of the model; HadAM3B45. This incorporates the same atmospheric component as HadCM3B but with prescribed sea surface temperatures (SSTs). The model incorporates the Met Office Surface Exchange Scheme (MOSES) version 2.191 which simulates water and energy fluxes and physiological processes such as photosynthesis, transpiration and respiration which is determined by stomatal conductance and consequently CO2 concentration. The fractional coverage of nine surface types are incorporated by MOSES 2.1 and simulated by the dynamic global vegetation model (DGVM) TRIFFID. Of the nine surface types, five are plant functional types (PFTs); deciduous and needleleaf trees, C3 and C4 grasses, shrubs, with the residual assigned to bare soil. Vegetation evolves throughout a model simulation depending on temperature, moisture, CO2 and competition with other PFTs. HadCM3B does not include an interactive carbon/methane cycle or ice model, so these boundary conditions have been imposed. The model was tuned using a Bayesian statistical methodology targeted on seven observational targets. The performance of the model was quantified in a probabilistic sense that accounts for structural error in the model—that is the conditioning was specifically designed not to expect a perfect model—since this could easily lead to the right answer for the wrong reasons. Five of the tuning targets are for the present day: annual mean temperature, tropopause water vapour, central and north Africa annual mean precipitation and present-day tropical vegetation cover. The remaining two targets are derived from the mid-Holocene: pollen-inferred North African 6 kyr BP precipitation and vegetation cover. The paleo-conditioned model was then validated43 using a leaf-wax precipitation record14 and other records. Together these updates allow a dynamic simulation of the Holocene greening of the Sahara that shows many similarities with the reconstructions of this time period. Armstrong (2023) North African humid periods over the past 800,000 years 3.1 Greening of Sahara Armstrong Abstract The Sahara region has experienced periodic wet periods over the Quaternary and beyond. These North African Humid Periods (NAHPs) are astronomically paced by precession which controls the intensity of the African monsoon system. However, most climate models cannot reconcile the magnitude of these events and so the driving mechanisms remain poorly constrained. Here, we utilise a recently developed version of the HadCM3B coupled climate model that simulates 20 NAHPs over the past 800 kyr which have good agreement with NAHPs identified in proxy data. Our results show that precession determines NAHP pacing, but we identify that their amplitude is strongly linked to eccentricity via its control over ice sheet extent. During glacial periods, enhanced ice-albedo driven cooling suppresses NAHP amplitude at precession minima, when humid conditions would otherwise be expected. This highlights the importance of both precession and eccentricity, and the role of high latitude processes in determining the timing and amplitude of the NAHPs. This may have implications for the out of Africa dispersal of plants and animals throughout the Quaternary. Armstrong (2023) North African humid periods over the past 800,000 years Larrasoaña Abstract Astronomically forced insolation changes have driven monsoon dynamics and recurrent humid episodes in North Africa, resulting in green Sahara Periods (GSPs) with savannah expansion throughout most of the desert. Despite their potential for expanding the area of prime hominin habitats and favouring out-of-Africa dispersals, GSPs have not been incorporated into the narrative of hominin evolution due to poor knowledge of their timing, dynamics and landscape composition at evolutionary timescales. We present a compilation of continental and marine paleoenvironmental records from within and around North Africa, which enables identification of over 230 GSPs within the last 8 million years. By combining the main climatological determinants of woody cover in tropical Africa with paleoenvironmental and paleoclimatic data for representative (Holocene and Eemian) GSPs, we estimate precipitation regimes and habitat distributions during GSPs. Their chronology is consistent with the ages of Saharan archeological and fossil hominin sites. Each GSP took 2–3 kyr to develop, peaked over 4–8 kyr, biogeographically connected the African tropics to African and Eurasian mid latitudes, and ended within 2–3 kyr, which resulted in rapid habitat fragmentation. We argue that the well-dated succession of GSPs presented here may have played an important role in migration and evolution of hominins. Larrasoaña (2013) Dynamics of Green Sahara Periods and Their Role in Hominin Evolution 3.2 Mimi Framework Mimi: An Integrated Assessment Modeling Framework Mimi is a Julia package for integrated assessment models developed in connection with Resources for the Future’s Social Cost of Carbon Initiative. Several models already use the Mimi framework, including those linked below. A majority of these models are part of the Mimi registry as detailed in the Mimi Registry subsection of this website. Note also that even models not registerd in the Mimi registry may be constructed to operate as packages. These practices are explained further in the documentation section “Explanations: Models as Packages”. MimiBRICK.jl MimiCIAM.jl MimiDICE2010.jl MimiDICE2013.jl MimiDICE2016.jl (version R not R2) MimiDICE2016R2.jl MimiFAIR.jl MimiFAIR13.jl MimiFAIRv1_6_2.jl MimiFAIRv2.jl MimiFUND.jl MimiGIVE.jl MimiHECTOR.jl MimiIWG.jl MimiMAGICC.jl MimiMooreEtAlAgricultureImpacts.jl Mimi_NAS_pH.jl mimi_NICE MimiPAGE2009.jl MimiPAGE2020.jl MimiRFFSPs.jl MimiRICE2010.jl Mimi-SNEASY.jl MimiSSPs.jl AWASH PAGE-ICE RICE+AIR Mimi Mimi.jl (GitHub) BerBastien 3.3 MODTRAN MODTRAN Demo Benestad Kininmonth used MODTRAN, but he must show how MODTRAN was used to arrive at figures that differ from other calculations, which also use MODTRAN. It is an important principle in science that others can repeat the same calculations and arrive at the same answer. You can play with MODTRAN on its website, but it is still important to explain how you arrive at your answers. Benestad(2022) ew misguided interpretations of the greenhouse effect from William Kininmonth 3.4 Monash Climate Model Some text on Monash Model 3.5 PAGE Kikstra Abstract A key statistic describing climate change impacts is the ‘social cost of carbon dioxide’ (SCCO 2 ), the projected cost to society of releasing an additional tonne of CO 2 . Cost-benefit integrated assessment models that estimate the SCCO 2 lack robust representations of climate feedbacks, economy feedbacks, and climate extremes. We compare the PAGE-ICE model with the decade older PAGE09 and find that PAGE-ICE yields SCCO 2 values about two times higher, because of its climate and economic updates. Climate feedbacks only account for a relatively minor increase compared to other updates. Extending PAGE-ICE with economy feedbacks demonstrates a manifold increase in the SCCO 2 resulting from an empirically derived estimate of partially persistent economic damages. Both the economy feedbacks and other increases since PAGE09 are almost entirely due to higher damages in the Global South. Including an estimate of interannual temperature variability increases the width of the SCCO 2 distribution, with particularly strong effects in the tails and a slight increase in the mean SCCO 2 . Our results highlight the large impacts of climate change if future adaptation does not exceed historical trends. Robust quantification of climate-economy feedbacks and climate extremes are demonstrated to be essential for estimating the SCCO 2 and its uncertainty. Kikstra Memo How temperature rises affect long-run economic output is an important open question (Piontek et al 2021). Climate impacts could either trigger addi- tional GDP growth due to increased agricultural productivity and rebuilding activities (Stern 2007, Hallegatte and Dumas 2009, Hsiang 2010, National Academies of Sciences Engineering and Medicine 2017) or inhibit growth due to damaged capital stocks (Pindyck 2013), lower savings (Fankhauser and Tol 2005) and inefficient factor reallocation (Piontek et al 2019). Existing studies have identified substan- tial impacts of economic growth feedbacks (Moyer et al 2014, Dietz and Stern 2015, Estrada et al 2015, Moore and Diaz 2015), but have not yet quantified the uncertainties involved based on empirical distri- butions. One particular example is Kalkuhl and Wenz (2020), who incorporate short-term economic per- sistence into a recent version of DICE (Nordhaus 2017), approximately tripling the resulting SCCO 2 ($37–$132). For fairly comparable economic assump- tions, the effect of long-term persistence is shown to increase the outcome even more ($220–$417) (Moore and Diaz 2015, Ricke et al 2018). We further expand on this work by deriving an empirical distribution of the persistence of climate impacts on economic growth based on recent developments (Burke et al 2015, Bastien-Olvera and Moore 2021) which we use to moderate GDP growth through persistent market damages. This partial persistence model builds upon recent empirical insights that not all contemporary economic damages due to climate change might be recovered in the long run (Dell et al 2012, Burke et al 2015, Kahn et al 2019, Bastien-Olvera and Moore 2021). Investigating how the SCCO 2 varies as a func- tion of the extent of persistence reveals a sensitivity that is on par with the heavily discussed role of dis- counting (Anthoff et al 2009b). Climatic extremes are another particularly important driver of climate change-induced dam- ages (Field et al 2012, Kotz et al 2021). The impact of interannual climate variability on the SCCO 2 has, however, not been analyzed previously, despite its clear economic implications (Burke et al 2015, Kahn et al 2019, Kumar and Khanna 2019) and an appar- ent relation to weather extremes such as daily min- ima and maxima (Seneviratne et al 2012), extreme rainfall (Jones et al 2013), and floods (Marsh et al 2016). Omission of such features in climate-economy models risks underestimation of the SCCO 2 because if convex regional temperature damage functions (Burke et al 2015) and an expected earlier cross- ing of potential climate and social thresholds in the climate-economy system (Tol 2019, Glanemann et al 2020). Here, we include climate variability by coupling the empirical temperature-damage func- tion with variable, autoregressive interannual tem- peratures. Increasing the amount of uncertainty by adding variable elements naturally leads to a less con- strained estimate for climate-driven impacts. How- ever, it is important to explore the range of possible futures, including the consideration of extremes in the climate-economy system (Otto et al 2020). In summary, we extend the PAGE-ICE CB-IAM (Yumashev et al 2019) to quantify the effect on the SCCO 2 of including possible long-term tem- perature growth feedback on economic trajectories, mean annual temperature anomalies, and the already modeled permafrost carbon and surface albedo feed- backs. Together, these provide an indication of the magnitude and uncertainties of the contribution of climate and economy feedbacks and interannual vari- ability to the SCCO 2 . Figure: Illustrative sketch of changes and extensions to PAGE-ICE presented in this paper. (a) Changes in the climate representation. PAGE-ICE includes a more detailed representation of CO 2 and CH 4 sinks, permafrost carbon feedback, the effect of sea ice and land snow decline on surface albedo, and a fat-tailed distribution of sea level rise. Here we also include interannual temperature variability with a temperature feedback through annual auto-correlation. (b) Changes in the damage module. The PAGE-ICE discontinuity damage component was reduced to correspond with updates to climate tipping points and sea-level rise risk, and market damages were recalibrated to an empirical estimate based on temperatures. Thus, while the discontinuity and non-economic damages continue to be calculated based on the separation between tolerable and excess temperature, the market damages are now calculated based on absolute temperature. Here we also extend PAGE-ICE with the possibility of persistent climate-induced damages, which in turn affects GDP pathways and scales emissions accordingly (feedback loop in the figure). The original PAGE- ICE does not simulate damage persistence. Thus, the economy always returns to the exogenous economic growth path, no matter how high the contemporary damages. Our setup recognizes that deterministic assessments of the SCCO 2 carry only very limited information. PAGE-ICE uses Monte Carlo sampling of over 150 parameter distributions (Yumashev et al 2019) to provide distributions of the results. All results presen- ted use 50 000 Monte Carlo draws (and 100 000 for PAGE09, using (RISK?) within Excel), with draws taken from the same superset to be able to compare SCCO 2 distributions across models. The PAGE-ICE model has been translated into the Mimi mod- eling framework, using the same validation pro- cess as for Mimi-PAGE (Moore et al 2018). Model code and documentation are available from the GitHub repository, here. To estimate the marginal damage of an additional tonne of CO 2 , PAGE-ICE is run twice, with one run following the exogenously specified emission pathway and the second run adding a CO 2 pulse. The SCCO 2 is then calculated as the difference in global equity- weighted damages between those two runs divided by the pulse size, discounted to the base year (2015). Equity weighting of damages follows the approach by Anthoff et al (2009a) using a mean (minimum, maximum) elasticity of marginal utility of consump- tion of 1.17 (0.1–2.0), and equity-weighted damages are discounted using a pure time preference rate of 1.03% (0.5%, 2.0%). For all our results, we rely on a 75Gt pulse size in the first time period of PAGE- ICE (mid-2017–2025), representing an annual pulse size of 10 Gt CO 2 . In this setup, we found that the choice of pulse size can have an effect on the SCCO 2 estimates. We implement the persistence parameter following Estrada et al (2015) into the growth system of Burke et al (2015) such that: \\[GDP_{r , t} = GDP_{r , t − 1} · (1 + g_{r , t − ρ} · γ_{r , t − 1} )\\], where g is the growth rate, γ represents the contemporary economic damages in % of GDP returned by the market damage function and ρ specifies the share of economic damages that persist and thus alter the growth trajectory in the long run. Note that this approach nests the extreme assumptions of zero persistence usually made in CB-IAMs. We also rescale green-house gas emissions proportionally to the change in GDP, such that emission intensities of economic output remain unchanged. Kikstra Conclusions Our results show that determining the level of per- sistence of economic damages is one of the most important factors in calculating the SCCO 2 , and our empirical estimate illustrates the urgency of increas- ing adaptive capacity, while suggesting that the mean estimate for the SCCO 2 may have been strongly underestimated. It further indicates that considering annual temperature anomalies leads to large increases in uncertainty about the risks of climate change. Differences between PAGE09 and PAGE-ICE show that the previous SCCO 2 results have also decidedly underestimated damages in the Global South. The implemented climate feedbacks and annual mean temperature variability do not have large effects on the mean SCCO 2 . The inclusion of permafrost thawing and surface albedo feedbacks is shown to lead to a relatively small increase in the SCCO 2 for SSP2- 4.5, with modest distributional effects. Consideration of temperature anomalies shows that internal vari- ability in the climate system can lead to increases in SCCO 2 estimates, and is key to understanding uncer- tainties in the climate-economy system, stressing the need for a better representation of variability and extremes in CB-IAMs. Including an empirical estimate of damage per- sistence demonstrates that even minor departures from the assumption that climate shocks do not affect GDP growth have major economic implications and eclipse most other modeling decisions. It suggests the need for a strong increase in adaptation to per- sistent damages if the long-term social cost of emis- sions is to be limited. Our findings corroborate that economic uncertainty is larger than climate science uncertainty in climate-economy system analysis (Van Vuuren et al 2020), and provide a strong argument that the assumption of zero persistence in CB-IAMs should be subject to increased scrutiny in order to avoid considerable bias in SCCO 2 estimates. Kikstra (2021) The social cost of carbon dioxide under climate-economy feedbacks and temperature variability (pdf) MIMI Modelling Framwork Mimi is a Julia package for integrated assessment models developed in connection with Resources for the Future’s Social Cost of Carbon Initiative. The source code for this package is located on Github here, and for detailed information on the installation and use of this package, as well as several tutorials, please see the Documentation. For specific requests for new functionality, or bug reports, please add an Issue to the repository. MIMI Home Page Kikstra Review Süd-Deutsche 3.6 Bern Simple Climate Model (BernSCM) Bern SCM Github README.md The Bern Simple Climate Model (BernSCM) is a free open source reimplementation of a reduced form carbon cycle-climate model which has been used widely in previous scientific work and IPCC assessments. BernSCM represents the carbon cycle and climate system with a small set of equations for the heat and carbon budget, the parametrization of major nonlinearities, and the substitution of complex component systems with impulse response functions (IRF). The IRF approach allows cost-efficient yet accurate substitution of detailed parent models of climate system components with near linear behaviour. Illustrative simulations of scenarios from previous multi-model studies show that BernSCM is broadly representative of the range of the climate-carbon cycle response simulated by more complex and detailed models. Model code (in Fortran) was written from scratch with transparency and extensibility in mind, and is provided as open source. BernSCM makes scientifically sound carbon cycle-climate modeling available for many applications. Supporting up to decadal timesteps with high accuracy, it is suitable for studies with high computational load, and for coupling with, e.g., Integrated Assessment Models (IAM). Further applications include climate risk assessment in a business, public, or educational context, and the estimation of CO2 and climate benefits of emission mitigation options. See the file BernSCM_manual(.pdf) for instructions on the use of the program. Strassmann 2017 The BernSCM Bern SCM (pdf) Bern SCM Github Code Parameters for tuning Bern UNFCCC Critics of Bern Model Y’know, it’s hard to figure out what the Bern model says about anything. This is because, as far as I can see, the Bern model proposes an impossibility. It says that the CO2 in the air is somehow partitioned, and that the different partitions are sequestered at different rates. For example, in the IPCC Second Assessment Report (SAR), the atmospheric CO2 was divided into six partitions, containing respectively 14%, 13%, 19%, 25%, 21%, and 8% of the atmospheric CO2. Each of these partitions is said to decay at different rates given by a characteristic time constant “tau” in years. (See Appendix for definitions). The first partition is said to be sequestered immediately. For the SAR, the “tau” time constant values for the five other partitions were taken to be 371.6 years, 55.7 years, 17.01 years, 4.16 years, and 1.33 years respectively. Now let me stop here to discuss, not the numbers, but the underlying concept. The part of the Bern model that I’ve never understood is, what is the physical mechanism that is partitioning the CO2 so that some of it is sequestered quickly, and some is sequestered slowly? I don’t get how that is supposed to work. The reference given above says: CO2 concentration approximation The CO2 concentration is approximated by a sum of exponentially decaying functions, one for each fraction of the additional concentrations, which should reflect the time scales of different sinks. So theoretically, the different time constants (ranging from 371.6 years down to 1.33 years) are supposed to represent the different sinks. Here’s a graphic showing those sinks, along with approximations of the storage in each of the sinks as well as the fluxes in and out of the sinks: (Carbon Cycle Picture) Now, I understand that some of those sinks will operate quite quickly, and some will operate much more slowly. But the Bern model reminds me of the old joke about the thermos bottle (Dewar flask), that poses this question: The thermos bottle keeps cold things cold, and hot things hot … but how does it know the difference? So my question is, how do the sinks know the difference? Why don’t the fast-acting sinks just soak up the excess CO2, leaving nothing for the long-term, slow-acting sinks? I mean, if some 13% of the CO2 excess is supposed to hang around in the atmosphere for 371.3 years … how do the fast-acting sinks know to not just absorb it before the slow sinks get to it? Anyhow, that’s my problem with the Bern model—I can’t figure out how it is supposed to work physically. Finally, note that there is no experimental evidence that will allow us to distinguish between plain old exponential decay (which is what I would expect) and the complexities of the Bern model. We simply don’t have enough years of accurate data to distinguish between the two. Nor do we have any kind of evidence to distinguish between the various sets of parameters used in the Bern Model. As I mentioned above, in the IPCC SAR they used five time constants ranging from 1.33 years to 371.6 years (gotta love the accuracy, to six-tenths of a year). But in the IPCC Third Assessment Report (TAR), they used only three constants, and those ranged from 2.57 years to 171 years. However, there is nothing that I know of that allows us to establish any of those numbers. Once again, it seems to me that the authors are just picking parameters. So … does anyone understand how 13% of the atmospheric CO2 is supposed to hang around for 371.6 years without being sequestered by the faster sinks? All ideas welcome, I have no answers at all for this one. I’ll return to the observational evidence regarding the question of whether the global CO2 sinks are “rapidly diminishing”, and how I calculate the e-folding time of CO2 in a future post. Best to all, APPENDIX: Many people confuse two ideas, the residence time of CO2, and the “e-folding time” of a pulse of CO2 emitted to the atmosphere. The residence time is how long a typical CO2 molecule stays in the atmosphere. We can get an approximate answer from Figure 2. If the atmosphere contains 750 gigatonnes of carbon (GtC), and about 220 GtC are added each year (and removed each year), then the average residence time of a molecule of carbon is something on the order of four years. Of course those numbers are only approximations, but that’s the order of magnitude. The “e-folding time” of a pulse, on the other hand, which they call “tau” or the time constant, is how long it would take for the atmospheric CO2 levels to drop to 1/e (37%) of the atmospheric CO2 level after the addition of a pulse of CO2. It’s like the “half-life”, the time it takes for something radioactive to decay to half its original value. The e-folding time is what the Bern Model is supposed to calculate. The IPCC, using the Bern Model, says that the e-folding time ranges from 50 to 200 years. On the other hand, assuming normal exponential decay, I calculate the e-folding time to be about 35 years or so based on the evolution of the atmospheric concentration given the known rates of emission of CO2. Again, this is perforce an approximation because few of the numbers involved in the calculation are known to high accuracy. However, my calculations are generally confirmed by those of Mark Jacobson as published here in the Journal of Geophysical Research. Eschenbach CO2 Lifetime The overall lifetime of CO 2 is updated to range from 30 to 95 years Any emission reduction of fossil-fuel particulate BC [Black Carbon] plus associated OM [Organic Matter] may slow global warming more than may any emission reduction of CO 2 or CH 4 for a specific period, Jacobsen Abstract This document describes two updates and a correction that affect two figures (Figures 1 and 14) in ‘‘Control of fossil-fuel particulate black carbon and organic matter, possibly the most effective method of slowing global warming’’ by Mark Z. Jacobson (Journal of Geophysical Research, 107(D19), 4410, doi:10.1029/2001JD001376, 2002). The modifications have no effect on the numerical simulations in the paper, only on the postsimulation analysis. The changes include the following: (1) The overall lifetime of CO 2 is updated to range from 30 to 95 years instead of 50 to 200 years, (2) the assumption that the anthropogenic emission rate of CO 2 is in equilibrium with its atmospheric mixing ratio is corrected, and (3) data for high-mileage vehicles available in the U.S. are used to update the range of mileage differences (15–30% better for diesel) in comparison with one difference previously (30% better mileage for diesel). The modifications do not change the main conclusions in J2002, namely, (1) ‘‘any emission reduction of fossil-fuel particulate BC plus associated OM may slow global warming more than may any emission reduction of CO 2 or CH 4 for a specific period,’’ and (2) diesel cars emitting continuously under the most recent U.S. and E.U. particulate standards (0.08 g/mi; 0.05 g/km) may warm climate per distance driven over the next 100+ years more than equivalent gasoline cars. Toughening vehicle particulate emission standards by a factor of 8 (0.01 g/mi; 0.006 g/km) does not change this conclusion, although it shortens the period over which diesel cars warm to 13–54 years,’’ except as follows: for conclusion 1, the period in Figure 1 of J2002 during which eliminating all fossil-fuel black carbon plus organic matter (f.f. BC + OM) has an advantage over all anthropogenic CO 2 decreases from 25–100 years to about 11–13 years and for conclusion 2 the period in Figure 14 of J2002 during which gasoline vehicles may have an advantage broadens from 13 to 54 years to 10 to &gt;100 years. On the basis of the revised analysis, the ratio of the 100-year climate response per unit mass emission of f.f. BC + OM relative to that of CO 2 -C is estimated to be about 90–190. Jacobsen (2002) (pdf) What’s Up with the Bern Model Mearns In modelling the growth of CO2 in the atmosphere from emissions data it is standard practice to model what remains in the atmosphere since after all it is the residual CO2 that is of concern in climate studies. In this post I turn that approach on its head and look at what is sequestered. This gives a very different picture showing that the Bern T1.2 and T18.5 time constants account for virtually all of the sequestration of CO2 from the atmosphere on human timescales (see chart below). The much longer T173 and T∞ processes are doing virtually nothing. Their principle action is to remove CO2 from the fast sinks, not from the atmosphere, in a two stage process that should not be modelled as a single stage. Given time, the slow sinks will eventually sequester 100% of human emissions and not 48% as the Bern model implies. Figure: The chart shows the amount of annual emissions removed by the various components of the Bern model. Unsurprisingly the T∞ component with a decline rate of 0% removes zero emissions and the T173 slow sink is not much better. Arguably, these components should not be in the model at all. The fast T1.2 and T18.5 sinks are doing all the work. The model does not handle the pre-1965 emissions decline perfectly, shown as underlying, but these too will be removed by the fast sinks and should also be coloured yellow and blue. Note that year on year the amount of CO2 removed has risen as partial P of CO2 has gone up. The gap between the coloured slices and the black line is that portion of emissions that remained in the atmosphere. The Bern Model for sequestration of CO2 from Earth’s atmosphere imagines the participation of a multitude of processes that are summarised into four time constants of 1.2, 18.5 and 173 years and one constant with infinity (Figure 1). I described it at length in this earlier post The Half Life of CO2 in Earth’s Atmosphere. Mearns 3.7 NorESM Norwegian Earth System Model About A climate model solves mathematically formulated natural laws on a three-dimensional grid. The climate model divides the soil system into components (atmosphere, sea, sea ice, land with vegetation, etc.) that interact through transmission of energy, motion and moisture. When the climate model also includes advanced interactive atmosphere chemistry and biogeochemical cycles (such as the carbon cycle), it is called an earth system model. The Norwegian Earth System Model NorESM has been developed since 2007 and has been an important tool for Norwegian climate researchers in the study of the past, present and future climate. NorESM has also contributed to climate simulation that has been used in research assessed in the IPCC’s fifth main report. INES The project Infrastructure for Norwegian Earth System Modeling (INES) will support the further development of NorESM and help Norwegian scientists also gain access to a cutting-edge earth system model in the years to come. Technical support will be provided for the use of a more refined grid, the ability to respond to climate change up to 10 years in advance, the inclusion of new processes at high latitudes and the ability of long-term projection of sea level. Climate simulations with NorESM are made on some of the most powerful supercomputers in Norway, and INES will help these exotic computers to be exploited in the best possible way and that the large data sets produced are efficiently stored and used. The project will ensure that researchers can efficiently use the model tool, analyze results and make the results available. 3.7.1 CCSM4 UCAR NCAR The University Corporation for Atmospheric Research (UCAR) is a US nonprofit consortium of more than 100 colleges and universities providing research and training in the atmospheric and related sciences. UCAR manages the National Center for Atmospheric Research (NCAR) and provides additional services to strengthen and support research and education through its community programs. Its headquarters, in Boulder, Colorado, include NCAR’s Mesa Laboratory. (Wikipedia) CCSM The Community Climate System Model (CCSM) is a coupled climate model for simulating Earth’s climate system. CCSM consists of five geophysical models: atmosphere (atm), sea-ice (ice), land (lnd), ocean (ocn), and land-ice (glc), plus a coupler (cpl) that coordinates the models and passes information between them. Each model may have “active,” “data,” “dead,” or “stub” component version allowing for a variety of “plug and play” combinations. During the course of a CCSM run, the model components integrate forward in time, periodically stopping to exchange information with the coupler. The coupler meanwhile receives fields from the component models, computes, maps, and merges this information, then sends the fields back to the component models. The coupler brokers this sequence of communication interchanges and manages the overall time progression of the coupled system. A CCSM component set is comprised of six components: one component from each model (atm, lnd, ocn, ice, and glc) plus the coupler. Model components are written primarily in Fortran 90. ccsm4 CESM The Community Earth System Model (CESM) is a fully-coupled, global climate model that provides state-of-the-art computer simulations of the Earth’s past, present, and future climate states. CESM2 is the most current release and contains support for CMIP6 experiment configurations. cesm models Simpler Models As part of CESM2.0, several dynamical core and aquaplanet configurations have been made available. Simpler Models 3.7.2 NorESM Features Despite the nationally coordinated effort, Norway has insufficient expertise and manpower to develop, test, verify and maintain a complete earth system model. For this reason, NorESM is based on the Community Climate System Model version 4, CCSM4, operated at the National Center for Atmospheric Research on behalf of the Community Climate System Model (CCSM)/Community Earth System Model (CESM) project of the University Corporation for Atmospheric Research. NorESM is, however, more than a model “dialect” of CCSM4. Notably, NorESM differs from CCSM4 in the following aspects: NorESM utilises an isopycnic coordinate ocean general circulation model developed in Bergen during the last decade originating from the Miami Isopycnic Coordinate Ocean Model (MICOM). The atmospheric module is modified with chemistry–aerosol–cloud–radiation interaction schemes developed for the Oslo version of the Community Atmosphere Model (CAM4-Oslo). Finally, the HAMburg Ocean Carbon Cycle (HAMOCC) model developed at the Max Planck Institute for Meteorology, Hamburg, adapted to an isopycnic ocean model framework, constitutes the core of the biogeochemical ocean module in NorESM. In this way NorESM adds to the much desired climate model diversity, and thus to the hierarchy of models participating in phase 5 of the Climate Model Intercomparison Project (CMIP5). In this and in an accompanying paper (Iversen et al., 2013), NorESM without biogeochemical cycling is presented. The reader is referred to Assmann et al. (2010) and Tjiputra et al. (2013) for a description of the biogeochemical ocean component and carbon cycle version of NorESM, respectively. There are several overarching objectives underlying the development of NorESM. Western Scandinavia and the surrounding seas are located in the midst of the largest surface temperature anomaly on earth governed by anomalously large oceanic and atmospheric heat transports. Small changes to these transports may result in large and abrupt changes in the local climate. To better understand the variability and stability of the climate system, detailed studies of the formation, propagation and decay of thermal and (oceanic) fresh water anomalies are required. NorESM is, as mentioned above, largely based on CCSM4. The main differences are the isopycnic coordinate ocean module in NorESM and that CAM4-Oslo substitutes CAM4 as the atmosphere module. The sea ice and land models in NorESM are basically the same as in CCSM4 and the Com- munity Earth System Model version 1 (CESM1), except that deposited soot and mineral dust aerosols on snow and sea ice are based on the aerosol calculations in CAM4-Oslo. 3.7.2.1 NorESM Aerosol Interactions The aerosol module is extended from earlier versions that have been published, and includes life-cycling of sea salt, mineral dust, particulate sulphate, black carbon, and primary and secondary organics. The impacts of most of the numer- ous changes since previous versions are thoroughly explored by sensitivity experiments. The most important changes are: modified prognostic sea salt emissions; updated treatment of precipitation scavenging and gravitational settling; inclu- sion of biogenic primary organics and methane sulphonic acid (MSA) from oceans; almost doubled production of land- based biogenic secondary organic aerosols (SOA); and in- creased ratio of organic matter to organic carbon (OM/OC) for biomass burning aerosols from 1.4 to 2.6. Compared with in situ measurements and remotely sensed data, the new treatments of sea salt and dust aerosols give smaller biases in near-surface mass concentrations and aerosol optical depth than in the earlier model version. The model biases for mass concentrations are approximately un- changed for sulphate and BC. The enhanced levels of mod- led OM yield improved overall statistics, even though OM is still underestimated in Europe and overestimated in North America. The global anthropogenic aerosol direct radiative forc- ing (DRF) at the top of the atmosphere has changed from a small positive value to −0.08 W m −2 in CAM4-Oslo. The sensitivity tests suggest that this change can be attributed to the new treatment of biomass burning aerosols and gravita- tional settling. Although it has not been a goal in this study, the new DRF estimate is closer both to the median model estimate from the AeroCom intercomparison and the best es- timate in IPCC AR4. Estimated DRF at the ground surface has increased by ca. 60 %, to −1.89 W m −2 The increased abundance of natural OM and the introduc- tion of a cloud droplet spectral dispersion formulation are the most important contributions to a considerably decreased es- timate of the indirect radiative forcing (IndRF). The IndRF is also found to be sensitive to assumptions about the coat- ing of insoluble aerosols by sulphate and OM. The IndRF of −1.2 W m −2 , which is closer to the IPCC AR4 estimates than the previous estimate of −1.9 W m −2 , has thus been obtained without imposing unrealistic artificial lower bounds on cloud droplet number concentrations. NorESM Bentsen (2013) NorESM - Part 1 (pdf) Iversen (2013) NorESM - Part 2 (pdf) Assmann (2010) Biogeochemical Ocean Component - Isopycnic (pdf) Tjiputra (2010) Carbon Cycle Component (pdf) Kirkevaag (2013) Aerosol-Climate Interactions (pdf) Community Earth System Model CESM "],["process-based-iams.html", "4 Process-based IAMs 4.1 The costs of “costless” climate mitigation 4.2 AI in Climate Research 4.3 Climate Catastrophe 4.4 Goal Index 4.5 Model Drift", " 4 Process-based IAMs Wilson Abstract Process-based integrated assessment models (IAMs) project long-term transformation path- ways in energy and land-use systems under what-if assumptions. IAM evaluation is necessary to improve the models’ usefulness as scientific tools applicable in the complex and contested domain of climate change mitigation. We contribute the first comprehensive synthesis of process-based IAM evaluation research, drawing on a wide range of examples across six different evaluation methods including historical simulations, stylised facts, and model diag- nostics. For each evaluation method, we identify progress and milestones to date, and draw out lessons learnt as well as challenges remaining. We find that each evaluation method has distinctive strengths, as well as constraints on its application. We use these insights to propose a systematic evaluation framework combining multiple methods to establish the appropriate- ness, interpretability, credibility, and relevance of process-based IAMs as useful scientific tools for informing climate policy. We also set out a programme of evaluation research to be mainstreamed both within and outside the IAM community. Wilson Memo Process-based IAMs have also been criticised for a range of perceived failings, including technological hubris, omitted drivers of sociotechnical change, and understating future uncertainties. We contribute the first synthesis of IAM evaluation research, drawing on a wide range of examples across six different evaluation methods: historical simulations, near- term observations, stylised facts, model hierarchies from simple to complex, model inter- comparison projects (including diagnostic indicators), and sensitivity analysis. For each method, we review key milestones in historical development and application, and draw out lessons learnt as well as remaining challenges. We also propose four criteria against which evaluation can help improve IAMs and their usefulness in policy contexts: appropriateness, interpretability, credibility, and relevance. We map each evaluation method onto these criteria, and conclude by arguing for a systematic evaluation framework which combines the strengths of multiple methods to overcome the limitations of any single method. Process-based IAMs not benefit-cost models We use ‘IAMs’ to mean process-based integrated assessment models (or what Weyant (2017) calls ‘detailed process’ or DP IAMs). These IAMs: Represent explicitly the drivers and processes of change in global energy and land-use systems linked to the broader economy, often with a high degree of technological resolution in the energy supply Capture both biophysical and socioeconomic processes including human preferences, but do not generally include future impacts or damages of climate change on these processes Project cost-effective ‘optimal’ mitigation pathways under what-if assumptions or subject to pre-defined outcomes such as limiting global warming to 2 °C. Many process-based IAMs originate in energy system models or energy-economy models which have since integrated land use, greenhouse gas emissions, and other climate-related processes. Wilson (2021) Evaluating process-based integrated assessment models of climate change mitigation (pdf) 4.1 The costs of “costless” climate mitigation Kotchen By holding to the position that there’s no such thing as a free lunch, economists may be overestimating the true cost of mitigation. This could occur in top-down models because of failure to recognize that inefficient decision-making exists for individuals, industry, and governments, meaning that there are opportunities to reduce GHG emissions while lowering costs—a so-called “win-win.” The question is how pervasive are such opportunities. Whereas the economics IAMs assume there are none, the IPCC asserts that win-wins can account for up to 16% of emission reductions in 2030. The IPCC estimate omits a whole category of difficult-to-quantify nonmonetary costs. Different starting points on costless mitigation explain the vast majority of the divergence in the IPCC’s bottom-up estimates and top-down economic approaches. We consider comparisons to six other IAMs often called “process-based” models because they focus more on bottom-up integration of energy and biophysical systems rather than on benefit-cost analysis. The models that we consider are also used in different portions of the IPCC analysis. We find that these models are more closely aligned with the bottom-up estimates than the benefit-cost IAMs with regard to mitigation potentials and the treatment of costless mitigation. This finding is important because whereas the IPCC’s bottom-up analysis acknowledges omission of indirect, nonmonetary costs, the same qualification is typically not associated with policy analysis coming out of the process-based models. Kotchen (2023) The costs of “costless” climate mitigation (pdf) Kotchen (2023) SM (pdf) 4.2 AI in Climate Research Irrgang Abstract Earth system models (ESMs) are our main tools for quantifying the physical state of the Earth and predicting how it might change in the future under ongoing anthropogenic forcing. In recent years, however, artificial intelligence (AI) methods have been increasingly used to augment or even replace classical ESM tasks, raising hopes that AI could solve some of the grand challenges of climate science. In this Perspective we survey the recent achievements and limitations of both process-based models and AI in Earth system and climate research, and propose a methodological transformation in which deep neural networks and ESMs are dismantled as individual approaches and reassembled as learning, self-validating and interpretable ESM–network hybrids. Following this path, we coin the term neural Earth system modelling. We examine the concurrent potential and pitfalls of neural Earth system modelling and discuss the open question of whether AI can bolster ESMs or even ultimately render them obsolete. Irrgang (2021) Towards neural Earth system modelling by integrating artificial intelligence in Earth system science (paywall) CarbonBrief A new line of climate research is emerging that aims to complement and extend the use of observations and climate models. We propose an approach whereby machine learning and climate models are not used as individual tools, but rather as connected “hybrids” that are capable of adaptive evolution and self-validation, while still being able to be interpreted by humans. Climate models have seen continuous improvement over recent decades. The most recent developments have seen the incorporation of biogeochemical cycles – the transfer of chemicals between living things and their environment – and how they interact with the climate system. Adding in new processes and greater detail have resulted in more sophisticated simulations of the Earth’s climate, it comes at the cost of increasingly large and complex models. ESMs are built on equations that represent the processes and interactions that drive the Earth’s climate. Some of these processes can be described by fundamental laws – such as the Navier-Stokes equations of fluid motion, which capture the speed, pressure, temperature and density of the gases in the atmosphere and the water in the ocean. However, others – such as physiological processes governing the vegetation that covers vast parts of the land surface – cannot and instead require approximations based on observations. These approximations – as well as other limitations that stem from the sheer complexity of the Earth system – introduce uncertainties into the model’s representation of the climate. As a result, despite the tremendous success of ESMs, some limitations remain – such as how well models capture the severity and frequency of extreme events, and abrupt changes and “tipping points”. In contrast to ESMs, machine learning does not require prior knowledge about the governing laws and relations within a problem. The respective relations are derived entirely from the data used during an automated learning process. This flexible and powerful concept can be expanded to almost any level of complexity. The availability of observed climate data and model simulations in combination with ready-to-use machine learning tools – such as TensorFlow and Keras – have led to an explosion of machine learning studies in Earth and climate sciences. These have explored how machine learning can be applied to enhance or even replace classical ESM tasks. Despite wordings like “learning” and “artificial intelligence”, today’s machine learning applications in this field are far from intelligent and lack actual process knowledge. More accurately, they are highly specialised algorithms that are trained to solve very specific problems solely based on the problem-related presented data. Consequently, machine learning is often considered a black box that makes it hard to gather insights from. Similarly, it is often very difficult to validate machine learning in terms of physical consistency, even if their generated outputs may seem plausible. Many of today’s machine learning applications for climate sciences are proof-of-concept studies that work in a simplified environment – for example, with a spatial resolution much lower than in state-of-the-art ESMs or with a reduced number of physical variables. Thus, it remains to be seen how well machine learning can be scaled up to operational and reliable usage. Initially, machine learning in climate research was primarily used for automated analysis of patterns and relations in Earth observations. However, more recently, it has been increasingly targeted towards ESMs – for example, by taking over or correcting specific model components or by accelerating computationally demanding numerical simulations. This development has led to the concept of “hybrids” of ESMs and machine learning, which aim to combine their respective methodological advantages while minimising their limitations. For instance, the hybrid concept has been explored for analysing continental hydrology. Continuing this line of research will increasingly blend the so-far still strict line between process-based models and machine learning approaches. Figure: Illustration of the stages of bringing ESMs and machine learning together towards neural Earth system modelling. The left and right branches visualise the current efforts and goals for building weakly coupled hybrids (blue and yellow), which converge towards strongly coupled hybrids. Within machine learning, making a correct prediction for the wrong reasons can be termed taking a “shortcut”, or having a system description that is “underdetermined”. Taking shortcuts is increasingly likely within climate science because the data available to us from the observational record is short and biased towards recent decades. CarbonBrief Review 4.3 Climate Catastrophe 4.3.1 Endgame Kemp Abstract Prudent risk management requires consideration of bad-to-worst-case scenarios. Yet, for climate change, suchpotential futures are poorly understood. Could anthropo-genic climate change result in worldwide societal collapseor even eventual human extinction? At present, this is adangerously underexplored topic. Yet there are amplereasons to suspect that climate change could result in aglobal catastrophe. Analyzing the mechanisms for theseextreme consequences could help galvanize action, improveresilience, and inform policy, including emergency respon-ses. We outline current knowledge about the likelihood ofextreme climate change, discuss why understanding bad-to-worst cases is vital, articulate reasons for concern about cat-astrophic outcomes, define key terms, and put forward aresearch agenda. The proposed agenda covers four mainquestions: 1) What is the potential for climate change todrive mass extinction events? 2) What are the mechanismsthat could result in human mass mortality and morbidity? 3)What are human societies’ vulnerabilities to climate-triggered risk cascades, such as from conflict, political insta-bility, and systemicfinancial risk? 4) How can these multiplestrands of evidence—together with other global dangers—be usefully synthesized into an“integrated catastropheassessment”?Itistimeforthescientific community to grap-ple with the challenge of better understanding catastrophicclimate change. Kemp (2022) Climate Endgame: Exploring catastrophic climatechange scenarios (pdf) 4.4 Goal Index In economic modelling choice of goal index (utility) function matters. Daniel 20181 presents this figure: Fig. Optimal CO2-prices with increasing risk aversion for EZ vs CRRA utility specification. (From Daniel 2018) As one of the co-authors explain: ‘We where not able to get the Social Cost of Carbon (SCC) under $120’. That is for ‘reasonable risk aversion’, using EZ-utilities. The ‘standard’ specification - with CRRA - utilities ends up with SCC of $20 or below. \\[V_1 = A [\\tilde{C\\_t}, \\mu_t(V\\_{t+1})]\\] Specification of the Goal Index function may seem a trivial technical issue - no so! There exists a broad professional litterature and profound discussions on this matter - which might de difficult to dis-entangle. Let us begin with Frank Ramsey’s growth model from 1928, commonly known as the Ramsey-Cass-Koopmans model. \\(F(K,L)\\) is an aggregate production function with factors \\(K\\) (Capital) and \\(L\\) (Labour). 4.5 Model Drift Abstract Sausen A method is proposed for removing the drift of coupled atmosphere-ocean models, which in the past has often hindered the application of coupled models in climate response and sensitivity experiments. The ocean-atmosphere flux fields exhibit inconsistencies when evaluated separately for the individual sub-systems in independent, uncoupled mode equilibrium climate computations. In order to balance these inconsistencies a constant ocean-atmosphere flux correction field is introduced in the boundary conditions coupling the two sub-systems together. The method ensures that the coupled model operates at the reference climate state for which the individual model subsystems were designed without affecting the dynamical response of the coupled system in climate variability experiments. The method is illustrated for a simple two component box model and an ocean general circulation model coupled to a two layer diagnostic atmospheric model. Memo Barthel The coupling of different climate sub-systems poses a number of technical problems. An obvious problem arising from the different time scales is the synchronization or matching of the numerical integration of subsys- tems characterized by widely differing time steps. A more subtle problem is Model Drift When two general circulation models of the atmosphere and ocean are coupled together in a single model, it is generally found that the cou- pled system gradually drifts into a new climate equilibrium state which is far removed from the observed climate. The coupled model climate equilibrium may be so unrealistic (for example, with respect to sea ice extent, or the oceanic equa- torial current system) that climate response or sensitivity experiments relative to this state be- come meaningless. This occurs regularly even when the individual models have been carefully tested in detailed numerical experiments in the decoupled mode and have been shown to yield satisfactory simulations of the climate of the sepa- rate ocean or atmosphere sub-systems. The drift of the coupled model is clearly a sign that something is amiss with the models. Howev- er, we suggest that it is not necessary to wait with climate response and sensitivity experiments with coupled models unit all causes of model drift have been properly identified and removed. Model drift is, in fact, an extremely sensitive indi- cator of model imperfections. The fact that the equilibrium climate into which a coupled model drifts is unacceptably far removed from the real climate does not necessarily imply that the model dynamics are too unrealistic for the model to be applied for climate response and sensitivity ex- periments. One should therefore devise methods for separating the drift problem from the basically independent problem of determining the change of the simulated climate induced by a change in boundary conditions a n d / o r external forcing (cli- mate response), and from the question of the ef- fect of changes in the physical or numerical for- mulation of the model (model sensitivity). Flux Correction The separation of the mean climate simulation from the climate response or sensitiv- ity problem can be achieved for coupled models rather simply by an alternative technique, the flux correction method. The errors that result in a drift of the coupled model are compensated in this method by con- stant correction terms in the flux expressions by which the separate sub-system models are cou- pled together. The correction terms have no in- fluence on the dynamics of the system in climate response or sensitivity experiments, but ensure that the “working point” of the model lies close to the climate state for which the individual models were originally tuned. The basic principle of the flux correction method is to couple the atmosphere and the ocean in such a manner that in the unperturbed case each sub- system simulates its own mean climate in the same manner as in the uncoupled mode, but re- sponds fully interactively to the other sub-system in climate response or sensitivity experiments. Sausen (1988) Coupled Ocean-Atmosphere Model Drift Flux Correction (pdf) See Links to references↩︎ "],["model-evaluation.html", "5 Model-evaluation 5.1 Measuering Forcings 5.2 Modeling Feedbacks Interactions", " 5 Model-evaluation GCMeval: a tool for climate model ensemble evaluation The global climate models indicate quite a range of future outcomes in terms of precipitation and temperature. To account for that, regional scenarios need to use fairly large multi-model ensembles. GCM-eval 5.1 Measuering Forcings Earth is on a budget – an energy budget. Our planet is constantly trying to balance the flow of energy in and out of Earth’s system. But human activities are throwing that off balance, causing our planet to warm in response. Adding more components that absorb radiation – like greenhouse gases – or removing those that reflect it – like aerosols – throws off Earth’s energy balance, and causes more energy to be absorbed by Earth instead of escaping into space. This is called a radiative forcing, and it’s the dominant way human activities are affecting the climate. limate modelling predicts that human activities are causing the release of greenhouse gases and aerosols that are affecting Earth’s energy budget. Now, a NASA study has confirmed these predictions with direct observations for the first time: radiative forcings are increasing due to human actions, affecting the planet’s energy balance and ultimately causing climate change. The paper was published online March 25, 2021, in the journal Geophysical Research Letters. NASA’s Clouds and the Earth’s Radiant Energy System (CERES) project studies the flow of radiation at the top of Earth’s atmosphere. A series of CERES instruments have continuously flown on satellites since 1997. Each measures how much energy enters Earth’s system and how much leaves, giving the overall net change in radiation. That data, in combination with other data sources such as ocean heat measurements, shows that there’s an energy imbalance on our planet. But it doesn’t tell us what factors are causing changes in the energy balance. This study used a new technique to parse out how much of the total energy change is caused by humans. The researchers calculated how much of the imbalance was caused by fluctuations in factors that are often naturally occurring, such as water vapor, clouds, temperature and surface albedo (essentially the brightness or reflectivity of Earth’s surface). The researchers calculated the energy change caused by each of these natural factors, then subtracted the values from the total. The portion leftover is the radiative forcing. The team found that human activities have caused the radiative forcing on Earth to increase by about 0.5 Watts per square meter from 2003 to 2018. The increase is mostly from greenhouse gases emissions from things like power generation, transport and industrial manufacturing. Reduced reflective aerosols are also contributing to the imbalance. NASA Goddard Direct Observations of Forcings 5.2 Modeling Feedbacks Interactions Berwyn A new study categorizes climate feedback loops and the possibility they could push the climate past planetary tipping points. Recent climate projections may be underestimating the pace of global warming in an atmosphere damaged by greenhouse gas emissions, because the interaction of powerful climate feedback loops that can accelerate warming are not well-represented in key climate models, an international team of scientists concluded in a study (paywall) published today in the journal One Earth. Their findings suggest that efforts to reduce emissions require even more urgency to avoid worst-case climate outcomes, the team reported. We would like to see an IPCC special report that focuses on the many risky climate feedbacks and the possible acceleration towards planetary tipping points. Recent evaluations conclude that, if countries meet the emissions-reduction targets they’ve set for themselves, the average global temperature would warm 2.7 degrees Celsius from pre-industrial temperatures by 2100, which would have catastrophic impacts for people and ecosystems. But if some of the feedback loops shown in the new paper accelerate, warming could soar well above that level, toward 4 degrees Celsius, by the end of the century. The researchers examined 41 climate feedback loops and found 27 that significantly increase warming but may not be fully accounted for in climate models. The models often overlook the cumulative effect all of them together might have. We are particularly concerned about several biological feedback loops, including permafrost thawing, forest destruction, loss of soil carbon and smoldering peatlands. These feedbacks may contribute significantly to warming over the course of the century. The Arctic, warming now at four times the global average rate, shows how feedback loops can interact. Scientists know thawing permafrost releases greenhouse gases. A 2017 study showed the potential for carbon releases from the disintegration of an Alabama-sized area of permafrost. Researchers, however, don’t currently expect that process by itself to cause runaway warming in the next few decades. But Arctic sea ice is dwindling too, exposing more dark ocean water to absorb more heat, which leads to yet more ice melting. And the changes to sea ice extent and ocean surface temperatures affect the atmosphere above the sea and the permafrost. What researchers don’t yet fully understand or show in climate models is how all those different processes can amplify each other, and whether their interactions will lead to sudden and irreversible changes in the next few decades. Other climate feedbacks in the ocean could also accelerate global warming. The models used to calculate widely accepted global temperature increases, including those by the IPCC, give us the pathways towards holding the 1.5 degrees Celsius limit. The models assume that the ocean will continue to operate more or less in the same mode as today, in terms of heat uptake and in terms of the solubility of carbon dioxide. But the amplifying cycle of feedbacks could change that sooner than expected. The most recent science reports from the Intergovernmental Panel on Climate Change identify a handful of the most important feedbacks that could push the climate past tipping points, but since they still can’t be adequately assessed, the international science panel can’t determine their probability of happening, or accurately project how they will interact. “There’s a deeper picture here,” said co-author Tim Lenton, director of the Global Systems Institute at the University of Exeter. Lenton was deeply influenced by scientist James Lovelock’s Gaia hypothesis, which proposes that life interacts with its inorganic surroundings on the planet to form a self-regulating system that perpetuates the conditions required for it to persist. “That’s part of what some climate models may be missing,” he said. “Lovelock was a visionary in thinking about how humans and our actions are intertwined in that system, and of course, that’s also what the paper is about,” he said, “how to see ourselves as integral parts and actors in the system.” There has been an effort to bring a more faithful representation of the nuanced influences of life on the Earth into climate modeling, he said, but they aren’t yet fully integrated. Evidence shows that the damage from global warming is going up in a nonlinear way, so every tenth of a degree more warming will cause more damage than the previous tenth Berwyn (2023) Scientists Examine Dangerous Global Warming ‘Accelerators’ "],["values-in-climate-science.html", "6 Values in Climate Science", " 6 Values in Climate Science Pulkkinen Values play a role in the construction of climate change information. Science has its own values, including openness, objectivity and evidence-based thinking. However social values — fundamental views on what is good, right and important — guide a number of decisions in the construction, assessment and communication of information. This marks a departure from the traditional ‘value-free ideal’ of science, according to which social values should have a limited role in scientific research, while values that are epistemic (for example, precision and accuracy) are seen as legitimately influencing research. Developing an acute awareness of how methodological choices and broader aims advantage different interests forms the first step in effectively managing the influence of values. There is no neutral way of framing information. Scientific research cannot be value-free, and climate science is no exception. The value of values in climate science To date, values are not widely acknowledged or discussed within physical climate science. Yet, effective management of values in physical climate science is required for the benefit of both science and society. Values in multimodel-based assessments A great number of research questions in climate science are answered by combining results from global climate model simulations within a multimodel framework and/or by their integration with observations. Winsberg20 argues that an opaque, inscrutable tapestry of values lies behind such results, due to the models’ size and complexity, distributed epistemic agency and generative entrenchment of methodological choices. Any multimodel-based assessment must moreover deal with the questions of which models to include, and how to combine them. The extremes range from including all available models, for example, in a Coupled Model Intercomparison Project context, and applying a one-model-one-vote principle, to selecting a single or very few flagship models. The underlying question of what is a good (enough) model is made explicit in model selection and implicit in model weighting, and relies on value-laden choices of metrics that may favour one spatial scale or region over another, one process over another or one stakeholder interest over another. This applies also to the AR6 approach of using a constrained ensemble of emulators for future projections, where the constraints are chosen to be based on simulation of past warming, equilibrium climate sensitivity and transient climate response. Values in event attribution Event attribution in its broadest sense is the evaluation of the contribution of causal factors to observed events. Two different methodological approaches to event attribution in climate science have been at times fiercely debated: the so-called probabilistic approach and the storylines approach, which occupy different positions on a spectrum of what level of conditioning on the meteorological circumstances is appropriate. A focus of debate has been the treatment of uncertainty in the dynamic response to anthropogenic forcing, given that uncertainty in the thermodynamic response is generally much lower. It has been argued that the two sides fundamentally disagree about risk preferences. The proponents of the storylines approach are more concerned with false negatives (that is, falsely rejecting or underestimating anthropogenic influence on an event), and their methodology is supposedly less prone to this type of error, while it is the opposite for the probabilistic approach and its proponents. Both risk preferences, and hence preference for either methodology, are argued by Winsberg et al. to be motivated by values, in particular by the balance between valuing epistemic confidence and informativeness. Values in climate services Climate services provide climate information to assist decision-making, aiming to support adaptation, mitigation and risk management decisions. This can be influenced by the values of all parties involved. Maximizing the fit of the information provided to the needs of the service users includes, in particular, the consideration of the users’ value system. Parker and Lusk argue that a significant and feasible component is to match the risk preferences of the analysis to those of the users. This can be done by learning which types of errors the users find particularly undesirable; recognizing methodological choices that differ in the risk of these errors; and making those choices in consultation with the users. For on-demand climate services, the authors suggest the use of clear warnings about product limitations and uncertainties in anticipation of various risk preferences, which allow for user customization at the point of service. Otherwise, they propose the prioritization of those user groups that might suffer especially severe harms and have limited access to climate information, and call for clear communication of which choices are influenced by values and how. Pulkinen (2022) The value of values in climate science (pdf sharedit) "],["jra55--japanese-reanalysis-data.html", "7 JRA55 -Japanese Reanalysis Data", " 7 JRA55 -Japanese Reanalysis Data Hausfather The Japanese Meteorological Agency’s JRA-55. This is one of the two next-generation reanalysis products (the other being ECMWF’s ERA5) that uses data from satellites, weather balloons, airplanes, surface stations, buoys, and ships to feed into state of the art weather models that calculate global temperatures back in time. These reanalysis products differ from surface-only datasets like NASA’s GISTEMP or Berkeley Earth, but produce broadly similar estimates of global temperature over time. Unlike surface datasets, their ingestion of massive real-time data allows for much more rapid updates of global temperature estimates. We can get estimates of daily global average temperatures from JRA-55 or ERA5 from a few days ago, while we will have to wait until early October to get monthly September temperatures from traditional surface records. Summer 2023 Extremes Figure: Daily global temperature anomalies from the JRA-55 dataset for each day of the year between 1958 (when the reanalysis dataset starts) through a few days ago (Sep 21st 2023) at time of publication). The black line shows 2023 to-date, the red line shows the prior warmest year on record (2016), and the light grey lines show all other years in the dataset. The month of September is highlighted. Hausfather (2023) Visualizing a summer of extremes in 7 charts "],["about.html", "A About", " A About Dyre Haugen and Dyrehaugen are Webians for Jon Martin - self-owned Globian, Webian, Norwegian and Canarian with a background from industrial research policy, urban planning and economic development consulting on global, regional and urban scales. I am deeply concerned about the (insane) way humanity (i.e. capitalism) interfere with nature. In an effort to gain insights in how and why this happens stuff is collected from around the web and put together in a linked set of web-sites. The sites are operated as personal notebooks. However, these days things can be easily published to the benefit of others concerned with the same issues. But be aware - this is not polished for presentation or peer-reviewed for exactness. I offer you just to have a look at my ‘work-desk’ as it appears in the moment. Any comment or suggestion can be mailed to dyrehaugen@pm.me Currently migrating from twitter (@dyrehaugen) to Mastodon (@dyrehaugen@mastodon.online) Thanks for visiting! "],["links.html", "B Links", " B Links Current Dyrehaugen Sites: rcap - On Capitalism (loc) rclm - On Climate Change (loc) recs - On Economics (loc) rfin - On Finance (loc) rngy - On Energy (loc) renv - On Environment (loc) rsts - On Statistics (loc) rurb - On Urbanization (loc) rvar - On Varia (loc) rwsd - On Wisdom (loc) Blogs: rde - Blog in English (loc) rdn - Blog in Norwegian (loc) Discontinued: jdt - Collection (Jekyll) (loc) hdt - Collection (Hugo) (loc) Not listed: (q:) dhe dhn jrw56 (z:) rcsa rpad rstart "],["news.html", "C NEWS C.1 230908 Antarctica Polar Amplification C.2 230116 No US Green Monetary Policy - but EU? C.3 211104 Global CO2 emissions have been flat for a decade, new data reveals C.4 211104 Top climate scientists are sceptical that nations will rein in global warming C.5 210921 Microsoft CO2-removal C.6 210909 ORCA turned on - Iceland C.7 210715 Arctic Sea Ice at Record Low C.8 210526 Dutch Court against Shell C.9 210509 NDCs need 80% increase to 2°C C.10 210508 Young Legal Action C.11 210424 Earth’s Axis tilted by Melting Glaciers C.12 210410 CO2 and Methane surged in 2020 C.13 210404 Gas Sustainability C.14 210220 US SCC Update in Progress C.15 210215 Focus on Steel, Meat and Cement C.16 210127 10 New Insights in Climate Science 2020 C.17 210130 Adaptation Summit C.18 210118 Warming all anthropogenic C.19 21014 Globale Temperature 1880-2020 C.20 210104 Not so long lag? C.21 210102 Climate Finance Shadow Report 2020", " C NEWS C.1 230908 Antarctica Polar Amplification Antarctica is likely warming at almost twice the rate of the rest of the world and faster than climate change models are predicting, with potentially far-reaching implications for global sea level rise. Scientists analysed 78 Antarctic ice cores to recreate temperatures going back 1,000 years and found the warming across the continent was outside what could be expected from natural swings. In West Antarctica, a region considered particularly vulnerable to warming with an ice sheet that could push up global sea levels by several metres if it collapsed, the study found warming at twice the rate suggested by climate models. Climate scientists have long expected that polar regions would warm faster than the rest of the planet – a phenomenon known as polar amplification – and this has been seen in the Arctic. Antarctica was warming at a rate of between 0.22C and 0.32C per decade, compared to 0.18C per decade predicted by climate models. Part of the warming in Antarctica is likely being masked by a change in a pattern of winds – also thought to be linked to global heating and the loss of ozone over the continent – that has tended to reduce temperatures. Guardian (2023) Antarctica warming much faster than models predicted in ‘deeply concerning’ sign for sea levels C.2 230116 No US Green Monetary Policy - but EU? Jay Powell has said the Federal Reserve will not become a “climate policymaker”, as he mounted a full-throated defence of the US central bank’s independence from political influence. In a speech delivered on Tuesday, the Fed chair said the central bank must steer clear of issues outside its congressionally mandated purview and instead maintain a narrow focus on keeping consumer prices stable, fostering a healthy labour market and ensuring the safety of the country’s banking system. “It is essential that we stick to our statutory goals and authorities, and that we resist the temptation to broaden our scope to address other important social issues of the day,” he said at a conference hosted by Sweden’s central bank. “Without explicit congressional legislation, it would be inappropriate for us to use our monetary policy or supervisory tools to promote a greener economy or to achieve other climate-based goals.” He added: “We are not, and will not be, a ‘climate policymaker’.” At the same event, Isabel Schnabel, a member of the six-person executive board of the European Central Bank, advocated greater action to address climate change. The German economist pledged to “ensure that all of the ECB’s policies are aligned with the objectives of the Paris Agreement to limit global warming to well below 2C”. The ECB’s position is clear. It worries that high interest rates to control inflation will undermine the green transition by raising the cost of investing in wind, solar, hydrogen and other clean energies necessary for moving to a net zero carbon world. But ECB and Fed are aligned on two important issues: First, that the primary role of green intervention lies not with independent central banks but with governments. Powell said that “in a well-functioning democracy, important public policy decisions should be made, in almost all cases, by the elected branches of government”. Schnabel concurred, saying, “governments must remain in the lead in accelerating the green transition”. Second, they agree central banks have a role when supervising the banking system in ensuring commercial banks understand and manage financial risks from global warming. These include weather-related risks to infrastructure that banks have financed or fossil fuel assets that might become near-worthless in future. ESG on a Sunday C.3 211104 Global CO2 emissions have been flat for a decade, new data reveals Global carbon dioxide (CO2) emissions from fossil fuels and cement have rebounded by 4.9% this year, new estimates suggest, following a Covid-related dip of 5.4% in 2020. The Global Carbon Project (GCP) projects that fossil emissions in 2021 will reach 36.4bn tonnes of CO2 (GtCO2), only 0.8% below their pre-pandemic high of 36.7GtCO2 in 2019. The researchers say they “were expecting some sort of rebound in 2021” as the global economy bounced back from Covid-19, but that it was “bigger than expected”. While fossil emissions are expected to return to near-record levels, the study also reassesses historical emissions from land-use change, revealing that global CO2 output overall may have been effectively flat over the past decade. The 2021 GCP almost halves the estimate of net emissions from land-use change over the past two years – and by an average of 25% over the past decade. These changes come from an update to underlying land-use datasets that lower estimates of cropland expansion, particularly in tropical regions. Emissions from land-use change in the new GCP dataset have been decreasing by around 4% per year over the past decade, compared to an increase of 1.8% per year in the prior version. However, the GCP authors caution that uncertainties in land-use change emissions remain large and “this trend remains to be confirmed”. CarbonBrief C.4 211104 Top climate scientists are sceptical that nations will rein in global warming Nature conducted an anonymous survey of the 233 living IPCC authors last month and received responses from 92 scientists — about 40% of the group. Their answers suggest strong scepticism that governments will markedly slow the pace of global warming, despite political promises made by international leaders as part of the 2015 Paris climate agreement. Six in ten of the respondents said that they expect the world to warm by at least 3 °C by the end of the century, compared with what conditions were like before the Industrial Revolution. That is far beyond the Paris agreement’s goal to limit warming to 1.5–2 °C. Most of the survey’s respondents — 88% — said they think global warming constitutes a ‘crisis’, and nearly as many said they expect to see catastrophic impacts of climate change in their lifetimes. Nature C.5 210921 Microsoft CO2-removal In January this year, Microsoft made a major announcement: it had paid for the removal of 1.3 million tonnes of carbon dioxide from the atmosphere. Among its purchases were projects to expand forests in Peru, Nicaragua and the United States, as well as initiatives to regenerate soil across US farms. Microsoft will pay the Swiss firm Climeworks to operate a machine in Iceland that pulls CO2 from the air and injects it into the ground, where it mineralizes and turns to stone. The amount of CO2 to be removed is equivalent to about 11% of the annual emissions from Microsoft’s value chain; of this, the company will count less than half as being certified to officially compensate for its emissions. It is the largest corporate procurement of carbon removal so far. Microsoft did this as part of its 2020 commitment to slash its greenhouse-gas emissions to ‘net zero’ — as one of more than 120 nations and 1,500 companies to set such goals1. By 2030, the company will reduce its emissions by half or more, and will have 100% of its electricity consumption matched by zero-carbon energy purchases. It will electrify its vehicle fleet, stop using diesel for backup energy and reduce emissions across its value chain. Emissions that are harder to abate, including historical emissions, will be compensated for by withdrawing carbon from the atmosphere. The firm is levying an internal carbon tax across all types of greenhouse-gas emission. It has set up a US$1-billion fund to invest in carbon reduction and removal technologies, and partnerships to provide social and environmental benefits. The aim is that, by 2030, the company will be carbon negative. By 2050, it will have removed all of its emissions since it was founded in 1975. Here we summarize the lessons learnt from Microsoft’s carbon-removal efforts, along with those from another early corporate procurement — the $9-million purchases of carbon removal in 2020 and 2021 by the US–Irish financial-infrastructure company Stripe. Although these are just two companies’ efforts, they are the first significant open solicitations focused exclusively on carbon removal. We write as a team composed of Microsoft staff working on the company’s carbon-negative programme and research scientists who analyse carbon reduction and removal strategies. We highlight three ‘bugs’ in the current system: inconsistent definitions of net zero, poor measurement and accounting of carbon, and an immature market in CO2 removal and offsets. These challenges need to be overcome if the world is to reach net zero by mid-century. Nature C.6 210909 ORCA turned on - Iceland The world’s largest plant designed to suck carbon dioxide out of the air and turn it into rock has started running, the companies behind the project said on Wednesday. The plant, named Orca after the Icelandic word “orka” meaning “energy”, consists of four units, each made up of two metal boxes that look like shipping containers. Constructed by Switzerland’s Climeworks and Iceland’s Carbfix, when operating at capacity the plant will draw 4,000 tonnes of carbon dioxide out of the air every year, according to the companies. The climate crisis requires a new culture and politics, not just new tech Peter Sutoris Read more According to the US Environmental Protection Agency, that equates to the emissions from about 870 cars. The plant cost between US$10 and 15m to build, Bloomberg reported. To collect the carbon dioxide, the plant uses fans to draw air into a collector, which has a filter material inside. Once the filter material is filled with CO2, the collector is closed and the temperature is raised to release the CO2 from the material, after which the highly concentrated gas can be collected. The CO2 is then mixed with the water before being injected at a depth of 1,000 metres into the nearby basalt rock where it is mineralised. Guardian C.7 210715 Arctic Sea Ice at Record Low ARCTIC SEA ICE AT RECORD LOW for this time of year. This is an enormous source of amplifying feedback. Losing the remaining Arctic sea ice and its reflection of solar energy back to space would be equivalent to another one trillion tons of CO2. Peter Carter (twitter) C.8 210526 Dutch Court against Shell This is a real ruling: it includes Scope 3 emissions. Rechtspraak De rechtbank Den Haag beveelt Royal Dutch Shell (RDS) om via het concernbeleid van de Shell-groep de CO2-uitstoot eind 2030 terug te brengen tot netto 45% ten opzichte van het niveau van 2019. Rechtsspraak (Dutch) English Translation C.9 210509 NDCs need 80% increase to 2°C On current trends, the probability of staying below 2 °C of warming is only 5% Liu (2021) Nature (pdf) C.10 210508 Young Legal Action The young people taking their countries to court over climate inaction Children and young adults around the world are demanding action from governments on global heating and the ecological crisis, Guardian C.11 210424 Earth’s Axis tilted by Melting Glaciers Since the 1990s, the loss of hundreds of billions of tonnes of ice a year into the oceans resulting from the climate crisis has caused the poles to move in new directions. The direction of polar drift shifted from southward to eastward in 1995 and that the average speed of drift from 1995 to 2020 was 17 times faster than from 1981 to 1995. Since 1980, the position of the poles has moved about 4 metres in distance. The accelerated decline [in water stored on land] resulting from glacial ice melting is the main driver of the rapid polar drift after the 1990s. Guardian C.12 210410 CO2 and Methane surged in 2020 Levels of the two most important anthropogenic greenhouse gases, carbon dioxide and methane, continued their unrelenting rise in 2020 despite the economic slowdown caused by the coronavirus pandemic response. CO2 The global surface average for carbon dioxide (CO2), calculated from measurements collected at NOAA’s remote sampling locations, was 412.5 parts per million (ppm) in 2020, rising by 2.6 ppm during the year. The global rate of increase was the fifth-highest in NOAA’s 63-year record, following 1987, 1998, 2015 and 2016. The annual mean at NOAA’s Mauna Loa Observatory in Hawaii was 414.4 ppm during 2020. The economic recession was estimated to have reduced carbon emissions by about 7 percent during 2020. Without the economic slowdown, the 2020 increase would have been the highest on record, according to Pieter Tans, senior scientist at NOAA’s Global Monitoring Laboratory. Since 2000, the global CO2 average has grown by 43.5 ppm, an increase of 12 percent. The atmospheric burden of CO2 is now comparable to where it was during the Mid-Pliocene Warm Period around 3.6 million years ago, when concentrations of carbon dioxide ranged from about 380 to 450 parts per million. During that time sea level was about 78 feet higher than today, the average temperature was 7 degrees Fahrenheit higher than in pre-industrial times, and studies indicate large forests occupied areas of the Arctic that are now tundra. Methane Analysis of samples from 2020 also showed a significant jump in the atmospheric burden of methane, which is far less abundant but 28 times more potent than CO2 at trapping heat over a 100-year time frame. NOAA’s preliminary analysis showed the annual increase in atmospheric methane for 2020 was 14.7 parts per billion (ppb), which is the largest annual increase recorded since systematic measurements began in 1983. The global average burden of methane for December 2020, the last month for which data has been analyzed, was 1892.3 ppb. That would represent an increase of about 119 ppb, or 6 percent, since 2000. NOAA C.13 210404 Gas Sustainability **Scientifically Sustainable* The European Commission is attempting to finish its sustainable finance taxonomy, a landmark regulation that from next year will define what can be labelled as a sustainable investment in the EU. A leaked proposal for the rules, shared with EU states last week, would label as sustainable some gas plants that generate power and also provide heating or cooling. That came after the Commission’s original proposal – which denied natural gas-fuelled power plants a green label, following the recommendation of the bloc’s expert advisers – faced resistance from some EU countries. Nine members of the expert group advising the European Union on its sustainable finance rules have threatened to step down if Brussels pushes ahead with plans that they say would discredit its efforts to fight climate change. EU countries disagree on what role natural gas should play in meeting climate goals. Gas emits roughly half the CO2 of coal when burned in power plants, but gas infrastructure is associated with leaks of methane, a potent greenhouse gas. “The concept of what is scientifically sustainable, that’s really not for politicians to decide,” said Andreas Hoepner, a professor at University College Dublin who signed the letter. Reuters C.14 210220 US SCC Update in Progress In its 2013 revision of the SCC, the Obama IWG arrived at a central value of around US$50 per tonne of CO2 emitted in 2020 (all values expressed in today’s dollars). It also established a range for the SCC ($15–75) and presented an estimate at the 95th percentile ($150). The time is ripe for this update, That IWG did a careful job, but devastating storms and wildfires are now more common, and costs are mounting. Advances in attribution science mean that researchers can now link many more extreme weather events directly to climate change, and new econometric techniques help to quantify the dollar impacts. The monetary losses exceed the predictions of early models. The same goes for sea-level rise and many other types of damage. Plenty of scientific and economic judgements need to be made. These include how to deal with endemic uncertainties, including sudden and irreversible ‘tipping points’, such as ice-sheet collapses. Ethical questions must be considered, including the consequences for vulnerable communities and future generations. Revising the SCC will take extensive research. A 2017 study by the US National Academies of Sciences, Engineering, and Medicine proposed building a new climate-economy model based on modules — separate components that handle climate change, socio-economic projections, damages, valuation and discounting Other nations use widely different SCC values or overall approaches2. Germany’s 2020 guidance presented two values: €195 (US$235) and €680 ($820). Some countries instead establish a goal for emissions reductions (such as the United Kingdom’s 68% reduction by 2030 compared to 1990 levels) and then focus on minimizing the costs of achieving it, estimated at $20–100 per tonne of CO2. This is called a target-consistent approach. Wagner (Nature) C.15 210215 Focus on Steel, Meat and Cement Bill Gates has written about Climate Change. His assessment is that there is not the time, money or political will to reconfigure the energy sector in 10 years, and encouraging an impossible goal dooms the world to short-term measures that prove insufficient. Crucially, people need to radically change how they produce the worst climate offenders: steel, meat and cement. Making steel and cement accounts for roughly 10% of all global emissions, and beef alone 4%. Bill Gates C.16 210127 10 New Insights in Climate Science 2020 Someof which are: ● Earth’s temperature response to doubling the levels of carbon dioxide in the atmosphere is now better understood. While previous IPCC assessments have used an estimated range of 1.5–4.5°C, recent research now suggests a narrower range of 2.3–4.5°C. ● Emissions of greenhouse gases from permafrost will be larger than earlier projections because of abrupt thaw processes, which are not yet included in global climate models. ● Global plant biomass uptake of carbon due to CO 2 fertilization may be limited in the future by nitrogen and phosphorus. ● Rights-based litigation is emerging as a tool to address climate change. Moving forward, the latest research calls for innovative, imaginative, and transformative approaches to building sustainable and resilient human societies. For instance, by strengthening global cooperative frameworks and building new governance arrangements that can include bottom-up community initiatives. In the short term, we have a one-off opportunity to get on the right path by directing post-pandemic recovery spending to green investments. If the focus is instead on economic growth, with sustainability as an afterthought, it would jeopardize our ability to deliver on the Paris Agreement. Alarmingly, governments do not yet seem to be seizing the opportunity to shift towards low-carbon, healthier, and more resilient societies. futureearth (pdf) C.17 210130 Adaptation Summit Climate change adaptation seems to be a fairly new concept to many leaders. It were sometimes mix-ups with mitigation during the high-level talks. Mitigation and adaptation are both important and sometimes they overlap, so mix-ups are understandable. Climate adaptation involves many communities and disciplines (e.g. weather forecasting, climate services, regional climate modelling, “distillation“, disaster risk reduction). Financing is clearly needed for climate change adaptation. To ensure progress and avoid lofty visions without results on the ground, there may also be a need for tangible results and to show examples and demonstrations. One specific type discussed at the summit was “Early warning systems” which play an important role. But early warning systems, the way I understand them, don’t provide information about climate risks on longer timescales. Weather and climate – short and long timescales – are of course connected but nevertheless different Rasmus (2021) Adaptation Summit C.18 210118 Warming all anthropogenic Parties to the Paris Agreement agreed to holding global average temperature increases “well below 2 °C above pre-industrial levels and pursuing efforts to limit the temperature increase to 1.5 °C above pre-industrial levels”. Monitoring the contributions of human-induced climate forcings to warming so far is key to understanding progress towards these goals. Here we use climate model simulations from the Detection and Attribution Model Intercomparison Project, as well as regularized optimal fingerprinting, to show that anthropogenic forcings caused 0.9 to 1.3 °C of warming in global mean near-surface air temperature in 2010–2019 relative to 1850–1900, compared with an observed warming of 1.1 °C. Greenhouse gases and aerosols contributed changes of 1.2 to 1.9 °C and −0.7 to −0.1 °C, respectively, and natural forcings contributed negligibly. These results demonstrate the substantial human influence on climate so far and the urgency of action needed to meet the Paris Agreement goals. Nature (paywall) C.19 21014 Globale Temperature 1880-2020 The rate of global warming has accelerated in the past several years. The 2020 global temperature was +1.3°C (~2.3°F) warmer than in the 1880-1920 base period; global temperature in that base period is a reasonable estimate of ‘pre-industrial’ temperature. The six warmest years in the GISS record all occur in the past six years, and the 10 warmest years are all in the 21st century. Growth rates of the greenhouse gases driving global warming are increasing, not declining. [GISSTEMP 2020 Update] (https://mailchi.mp/caa/global-temperature-in-2020?e=96d59a909f) C.20 210104 Not so long lag? Until recently, Mann explained in The Guardian, scientists believed the climate system—a catch-all term for the interaction among the Earth’s atmosphere, oceans, and other parts of the biosphere—carried a long lag effect. This lag effect was mainly a function of carbon dioxide remaining in the atmosphere and trapping heat for many decades after being emitted. So, even if humanity halted all CO2 emissions overnight, average global temperatures would continue to rise for 25 to 30 years, while also driving more intense heat waves, droughts, and other climate impacts. Halting emissions will take at least twenty years, under the best of circumstances, and so humanity was likely locked in to at least 50 more years of rising temperatures and impacts. Research over the past ten years, however, has revised this vision of the climate system. Scientists used to “treat carbon dioxide in the atmosphere as if it was a simple control knob that you turn up” and temperatures climb accordingly, “but in the real world we now know that’s not what happens,” Mann said. Instead, if humans “stop emitting carbon right now … the oceans start to take up carbon more rapidly.” The actual lag effect between halting CO2 emissions and halting temperature rise, then, is not 25 to 30 years but, per Mann, “more like three to five years.” (October 2020) Guardian article Covering Climate Now article C.21 210102 Climate Finance Shadow Report 2020 Oxfam has released this report with subtitle Asessing progress towards the $100 billion commitment Progress is NOT in line with need or pledges. Climate change could undo decades of progress in development and dramatically increase global inequalities. There is an urgent need for climate finance to help countries cope and adapt. Over a decade ago, developed countries committed to mobilize $100bn per year by 2020 to support developing countries to adapt and reduce their emissions. The goal is a critical part of the Paris Agreement. As 2020 draws to a close, Oxfam’s Climate Finance Shadow Report 2020 offers an assessment of progress towards the $100bn goal. Based on 2017–18 reported numbers, developed countries are likely to claim they are on track to meet the $100bn goal. And on their own terms, they may be. But how the goal is met is as important as whether it is met. The dubious veracity of reported numbers, the extent to which climate finance is increasing developing country indebtedness, and the enduring gap in support for adaptation, LDCs and SIDS, are grave concerns. Meeting the $100bn goal on these terms would be cause for concern, not celebration. Oxfam Report (pdf) "],["sitelog.html", "D Sitelog", " D Sitelog Latest Additions December 25, 2023 HadCM3B\\        HadCM3B used to simulate green sahara periods December 25, 2023 HadCM3B\\ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
